{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"3_FOMC_Analysis_Preprocess_Text.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"FKOrbYlXS25G"},"source":["# Clean Text Data"]},{"cell_type":"code","metadata":{"id":"3FtIanACS25I"},"source":["# # Check module locations if required\n","# import sys\n","# import pprint\n","# pprint.pprint(sys.path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVP2jTzMS25I"},"source":["import numpy as np\n","import pandas as pd\n","import datetime as dt\n","\n","from lxml import etree\n","from dateutil.relativedelta import *\n","\n","import seaborn as sns; sns.set(style=\"darkgrid\")\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import re\n","import pickle\n","from tqdm.notebook import tqdm\n","\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdJnn0TpS25J"},"source":["# Set display preference (Optional)\n","plt.rcParams[\"figure.figsize\"] = (18,9)\n","plt.style.use('fivethirtyeight')\n","\n","pd.options.display.max_rows = 20\n","pd.options.display.max_seq_items = 50\n","pd.set_option('display.max_colwidth', 200)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMEO90U2S25J"},"source":["## Define Utility Functions"]},{"cell_type":"code","metadata":{"id":"CbSKPSywS25J"},"source":["# Functions for map() or apply()\n","\n","def get_word_count(x):\n","    '''\n","    Retun the number of words for the given text x.\n","    '''\n","    x = x.replace(\"[SECTION]\", \"\")\n","    return len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', x))\n","        \n","def get_rate_change(x):\n","    '''\n","    Returns rate change decision of the FOMC Decision for the given date x.\n","    x should be of datetime type or yyyy-mm-dd format string.\n","    '''\n","    # If x is string, convert to datetime\n","    if type(x) is str:\n","        try:\n","            x = dt.datetime.strptime(x, '%Y-%m-%d')\n","        except:\n","            return None\n","    \n","    if x in fomc_calendar.index:\n","        return fomc_calendar.loc[x]['RateDecision']\n","    else:        \n","        return None\n","\n","def get_rate(x):\n","    '''\n","    Returns rate of the FOMC Decision for the given date x.\n","    x should be of datetime type or yyyy-mm-dd format string.\n","    '''\n","    # If x is string, convert to datetime\n","    if type(x) is str:\n","        try:\n","            x = dt.datetime.strptime(x, '%Y-%m-%d')\n","        except:\n","            return None\n","        \n","    if x in fomc_calendar.index:\n","        return fomc_calendar.loc[x]['Rate']\n","    else:        \n","        return None\n","\n","def get_next_meeting_date(x):\n","    '''\n","    Returns the next fomc meeting date for the given date x, referring to fomc_calendar DataFrame.\n","    Usually FOMC Meetings takes two days, so it starts searching from x+2.\n","    x should be of datetime type or yyyy-mm-dd format string.\n","    '''\n","    # If x is string, convert to datetime\n","    if type(x) is str:\n","        try:\n","            x = dt.datetime.strptime(x, '%Y-%m-%d')\n","        except:\n","            return None\n","\n","    # Add two days to get the day after next\n","    x = x + dt.timedelta(days=2)\n","    \n","    # Just in case, sort fomc_calendar from older to newer\n","    fomc_calendar.sort_index(ascending=True, inplace=True)\n","    \n","    if fomc_calendar.index[0] > x:\n","        # If the date is older than the first FOMC Meeting, do not return any date.\n","        return None\n","    else:\n","        for i in range(len(fomc_calendar)):\n","            if x < fomc_calendar.index[i]:\n","                return fomc_calendar.index[i]\n","        # If x is greater than the newest FOMC meeting date, do not return any date.\n","        return None\n","    \n","def get_chairperson(x):\n","    '''\n","    Return a tuple of chairperson's Fullname for the given date x.\n","    '''\n","    # If x is string, convert to datetime\n","    if type(x) is str:\n","        try:\n","            x = dt.datetime.strftime(x, '%Y-%m-%d')\n","        except:\n","            return None\n","    \n","    chairperson = chairpersons.loc[chairpersons['FromDate'] <= x].loc[x <= chairpersons['ToDate']]\n","    return list(chairperson.FirstName)[0] + \" \" + list(chairperson.Surname)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgmpfJL2S25K"},"source":["def reorganize_df(df, doc_type):\n","    '''\n","    Reorganize the loaded dataframe, which has been obrained by FomcGetData for further processing\n","        - Add type\n","        - Add word count\n","        - Add rate, decision (for meeting documents, None for the others)\n","        - Add next meeting date, rate and decision\n","        - Copy contents to org_text\n","        - Remove line breaks from contents in text\n","        - Split contents by \"[SECTION]\" to list in text_sections\n","    '''\n","    \n","    if doc_type in ('statement', 'minutes', 'presconf_script', 'meeting_script'):\n","        is_meeting_doc = True\n","    elif doc_type in ('speech', 'testimony'):\n","        is_meeting_doc = False\n","    else:\n","        print(\"Invalid doc_type [{}] is given!\".format(doc_type))\n","        return None\n","    \n","    dict = {\n","        'type': doc_type,\n","        'date': df['date'],\n","        'title': df['title'],\n","        'speaker': df['speaker'],\n","        'word_count': df['contents'].map(get_word_count),\n","        'decision': df['date'].map(lambda x: get_rate_change(x) if is_meeting_doc else None),\n","        'rate': df['date'].map(lambda x: get_rate(x) if is_meeting_doc else None),\n","        'next_meeting': df['date'].map(get_next_meeting_date),\n","        'next_decision': df['date'].map(get_next_meeting_date).map(get_rate_change),\n","        'next_rate': df['date'].map(get_next_meeting_date).map(get_rate),        \n","        'text': df['contents'].map(lambda x: x.replace('\\n','').replace('\\r','').strip()),\n","        'text_sections': df['contents'].map(lambda x: x.replace('\\n','').replace('\\r','').strip().split(\"[SECTION]\")),\n","        'org_text': df['contents']\n","    }\n","\n","    new_df = pd.DataFrame(dict)\n","    new_df['decision'] = new_df['decision'].astype('Int8')\n","    new_df['next_decision'] = new_df['next_decision'].astype('Int8')\n","    print(\"No rate decision found: \", new_df['decision'].isnull().sum())\n","    print(\"Shape of the dataframe: \", new_df.shape)\n","    #new_df.dropna(subset=['decision'], axis=0, inplace=True)\n","    return new_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzCjogpoS25K"},"source":["# Split functions to process long text in machine learning based NLP\n","\n","def get_split(text, split_len=200, overlap=50):\n","    '''\n","    Returns a list of split text of $split_len with overlapping of $overlap.\n","    Each item of the list will have around split_len length of text.\n","    '''\n","    l_total = []\n","    words = re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n","    \n","    if len(words) < split_len:\n","        n = 1\n","    else:\n","        n = (len(words) - overlap) // (split_len - overlap) + 1\n","        \n","    for i in range(n):\n","        l_parcial = words[(split_len - overlap) * i: (split_len - overlap) * i + split_len]\n","        l_total.append(\" \".join(l_parcial))\n","    return l_total\n","\n","def get_split_df(df, split_len=200, overlap=50):\n","    '''\n","    Returns a dataframe which is an extension of an input dataframe.\n","    Each row in the new dataframe has less than $split_len words in 'text'.\n","    '''\n","    split_data_list = []\n","\n","    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","        #print(\"Original Word Count: \", row['word_count'])\n","        text_list = get_split(row[\"text\"], split_len, overlap)\n","        for text in text_list:\n","            row['text'] = text\n","            #print(len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)))\n","            row['word_count'] = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text))\n","            split_data_list.append(list(row))\n","            \n","    split_df = pd.DataFrame(split_data_list, columns=df.columns)\n","    split_df['decision'] = split_df['decision'].astype('Int8')\n","    split_df['next_decision'] = split_df['next_decision'].astype('Int8')\n","\n","    return split_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZ35IaXkS25L"},"source":["def remove_short_section(df, min_words=50):\n","    '''\n","    Using 'text_sections' of the given dataframe, remove sections having less than min_words.\n","    It concatinate sections with a space, which exceeds min_words and update 'text'.\n","    As a fallback, keep a text which concatinates sections having more than 20 words and use it\n","     if there is no section having more than min_words.\n","    If there is no sections having more than 20 words, remove the row.\n","    '''\n","    new_df = df.copy()\n","    new_text_list = []\n","    new_text_section_list = []\n","    new_wc_list = []\n","    \n","    for i, row in tqdm(new_df.iterrows(), total=new_df.shape[0]):\n","        new_text = \"\"\n","        bk_text = \"\"\n","        new_text_section = []\n","        bk_text_section = []\n","                \n","        for section in row['text_sections']:\n","            num_words = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', section))\n","            if num_words > min_words:\n","                new_text += \" \" + section\n","                new_text_section.append(section)\n","            elif num_words > 20:\n","                bk_text += \" \" + section\n","                bk_text_section.append(section)\n","                \n","        \n","        new_text = new_text.strip()\n","        bk_text = bk_text.strip()\n","        \n","        if len(new_text) > 0:\n","            new_text_list.append(new_text)\n","            new_text_section_list.append(new_text_section)\n","        elif len(bk_text) > 0:\n","            new_text_list.append(bk_text)\n","            new_text_section_list.append(bk_text_section)\n","        else:\n","            new_text_list.append(\"\")\n","            new_text_section_list.append(\"\")\n","        \n","        # Update the word count\n","        new_wc_list.append(len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', new_text_list[-1])))\n","        \n","    new_df['text'] = new_text_list\n","    new_df['word_count'] = new_wc_list\n","    \n","    return new_df.loc[new_df['word_count'] > 0]\n","\n","def remove_short_nokeyword(df, keywords = ['rate', 'rates', 'federal fund', 'outlook', 'forecast', 'employ', 'economy'], min_times=2, min_words=50):\n","    '''\n","    Drop sections which do not have any one of keywords for min_times times\n","     before applying remove_short_section()\n","    '''\n","    \n","    new_df = df.copy()\n","    new_section_list = []\n","    \n","    for i, row in tqdm(new_df.iterrows(), total=new_df.shape[0]):\n","        new_section = []\n","                \n","        for section in row['text_sections']:\n","            if len(set(section.split()).intersection(keywords)) >= min_times:\n","                new_section.append(section)\n","        \n","        new_section_list.append(new_section)\n","    \n","    new_df['text_sections'] = new_section_list\n","    \n","    return remove_short_section(new_df, min_words=min_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsM17lRTS25L"},"source":["## Load Data"]},{"cell_type":"markdown","metadata":{"id":"9jETiGFAS25M"},"source":["### Chairpersons"]},{"cell_type":"code","metadata":{"id":"Qr82ivYVS25M"},"source":["# FOMC Chairperson's list\n","chairpersons = pd.DataFrame(\n","    data=[[\"Volcker\", \"Paul\", dt.datetime(1979,8,6), dt.datetime(1987,8,10)],\n","          [\"Greenspan\", \"Alan\", dt.datetime(1987,8,11), dt.datetime(2006,1,31)], \n","          [\"Bernanke\", \"Ben\", dt.datetime(2006,2,1), dt.datetime(2014,1,31)], \n","          [\"Yellen\", \"Janet\", dt.datetime(2014,2,3), dt.datetime(2018,2,3)],\n","          [\"Powell\", \"Jerome\", dt.datetime(2018,2,5), dt.datetime(2022,2,5)]],\n","    columns=[\"Surname\", \"FirstName\", \"FromDate\", \"ToDate\"])\n","chairpersons"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vWC2iAkS25N"},"source":["### Load Calendar"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Y0-nG8J7S25N"},"source":["file = open('../data/preprocessed/fomc_calendar.pickle', 'rb')\n","\n","fomc_calendar = pickle.load(file)\n","file.close()\n","\n","print(fomc_calendar.shape)\n","fomc_calendar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VJ2W7GX7S25O"},"source":["# #Check caleander\n","# fomc_calendar.loc[fomc_calendar.index >= dt.datetime(1998, 1, 27)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jW535s-qS25O"},"source":["### Statement"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Ckc3R7EaS25O"},"source":["file = open('../data/FOMC/statement.pickle', 'rb')\n","\n","statement_df = pickle.load(file)\n","file.close()\n","\n","print(statement_df.shape)\n","statement_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2lRWI3fS25O"},"source":["# Sample Contents - the 2nd last\n","print(statement_df['contents'].iloc[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5lyomDwS25O"},"source":["### Meeting Minutes"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"LBOIwdaFS25P"},"source":["file = open('../data/FOMC/minutes.pickle', 'rb')\n","\n","minutes_df = pickle.load(file)\n","file.close()\n","\n","print(minutes_df.shape)\n","minutes_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"77VLhtLlS25P"},"source":["# Sample Contents - the 2nd last\n","print(minutes_df['contents'].iloc[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DulvObmzS25P"},"source":["### Meeting Transcripts"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"bn71Q79pS25P"},"source":["file = open('../data/FOMC/meeting_script.pickle', 'rb')\n","\n","meeting_script_df = pickle.load(file)\n","file.close()\n","\n","print(meeting_script_df.shape)\n","meeting_script_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMQ5gZLrS25Q"},"source":["# Sample Contents - the 2nd last\n","print(meeting_script_df['contents'].iloc[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7q_ERFp2S25Q"},"source":["### Press Conference Transcripts"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"dXh5D_KHS25R"},"source":["file = open('../data/FOMC/presconf_script.pickle', 'rb')\n","\n","presconf_script_df = pickle.load(file)\n","file.close()\n","\n","print(presconf_script_df.shape)\n","presconf_script_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g4gYYjK-S25R"},"source":["# Sample Contents - the 2nd last\n","print(presconf_script_df['contents'].iloc[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2fG0yTsS25R"},"source":["### Speech"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AvLUwE4OS25S"},"source":["file = open('../data/FOMC/speech.pickle', 'rb')\n","\n","speech_df = pickle.load(file)\n","file.close()\n","\n","print(speech_df.shape)\n","speech_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMrHNFL8S25S"},"source":["# Sample Contents - the 2nd last\n","print(speech_df['contents'].iloc[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OeU2k6EeS25S"},"source":["### Testimony"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Wodr8CAcS25S"},"source":["file = open('../data/FOMC/testimony.pickle', 'rb')\n","\n","testimony_df = pickle.load(file)\n","file.close()\n","\n","print(testimony_df.shape)\n","testimony_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iqIjM38S25T"},"source":["# Sample Contents - the 2nd last\n","print(testimony_df['contents'].iloc[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AY1zS_0MS25T"},"source":["## Statements DataFrame"]},{"cell_type":"code","metadata":{"id":"XD4wnuc5S25T"},"source":["statement_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOfEQL5lS25T"},"source":["### Add Quantitative Easing as a Lower event"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"-WUH6YMWS25T"},"source":["# Add When QE was first announced on 2008-11-25. No statemenet on that day.\n","\n","if statement_df.loc[statement_df['date'] == dt.datetime.strptime('2008-11-25', '%Y-%m-%d')].shape[0] == 0:\n","    qe_text = \"The Federal Reserve announced on Tuesday that it will initiate a program \"\\\n","              \"to purchase the direct obligations of housing-related government-sponsored \"\\\n","              \"enterprises (GSEs)--Fannie Mae, Freddie Mac, and the Federal Home Loan Banks \"\\\n","              \"--and mortgage-backed securities (MBS) backed by Fannie Mae, Freddie Mac, \"\\\n","              \"and Ginnie Mae.  Spreads of rates on GSE debt and on GSE-guaranteed mortgages \"\\\n","              \"have widened appreciably of late.  This action is being taken to reduce the cost \"\\\n","              \"and increase the availability of credit for the purchase of houses, which in turn \"\\\n","              \"should support housing markets and foster improved conditions in financial markets \"\\\n","              \"more generally. Purchases of up to $100 billion in GSE direct obligations under \"\\\n","              \"the program will be conducted with the Federal Reserve's primary dealers through \"\\\n","              \"a series of competitive auctions and will begin next week.  Purchases of up to \"\\\n","              \"$500 billion in MBS will be conducted by asset managers selected via a competitive \"\\\n","              \"process with a goal of beginning these purchases before year-end.  \"\\\n","              \"Purchases of both direct obligations and MBS are expected to take place over \"\\\n","              \"several quarters.  Further information regarding the operational details of this \"\\\n","              \"program will be provided after consultation with market participants.\"\n","    statement_df = statement_df.append(\n","        pd.Series([dt.datetime.strptime('2008-11-25', '%Y-%m-%d'), qe_text, 'Ben Bernanke', 'FOMC statement'], index=statement_df.columns),\n","        ignore_index=True\n","    )\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVoSKvghS25T"},"source":["### Process the dataframe"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Otcs6yFRS25U"},"source":["# Reorganize the dataframe\n","proc_statement_df = reorganize_df(statement_df, 'statement')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"54DqdlwuS25U"},"source":["# Check the returned dataframe\n","proc_statement_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"az_C4tF6S25U"},"source":["# Check which row does not have rate\n","proc_statement_df.loc[proc_statement_df['rate'].isnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2he0-lCS25U"},"source":["x = dt.datetime.strptime('2019-01-01', '%Y-%m-%d')\n","fomc_calendar.loc[fomc_calendar.index > x]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"_NtHanMZS25U"},"source":["# Remove sections having less than 50 words because those are unlikely to contain meaningful sentences.\n","proc_statement_df = remove_short_section(proc_statement_df, min_words=50)\n","proc_statement_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iaw_L-eWS25V"},"source":["plt.figure(figsize=(10,5))\n","sns.distplot(proc_statement_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQve2eK4S25V"},"source":["### Split contents to max 200 words"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ZmZC_ynpS25V"},"source":["split_statement_df = get_split_df(proc_statement_df)\n","split_statement_df.tail(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZCahWCiES25V"},"source":["### Filter out by keywords"]},{"cell_type":"code","metadata":{"id":"e0uzz9V9S25V"},"source":["# Keep sections having keywords and long enough\n","keyword_statement_df = remove_short_nokeyword(proc_statement_df)\n","keyword_statement_df.reset_index(drop=True, inplace=True)\n","print(keyword_statement_df.shape)\n","keyword_statement_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqNIyJGsS25V"},"source":["# Drop text during the process to reduce the output size\n","proc_statement_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","split_statement_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","keyword_statement_df.drop(columns=['text_sections', 'org_text'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1j2dvfERS25W"},"source":["## Minutes DataFrame"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"mns5j8l8S25W"},"source":["minutes_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7gEUG8xFS25W"},"source":["# Reorganize the dataframe\n","proc_minutes_df = reorganize_df(minutes_df, 'minutes')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PhugRRrWS25W"},"source":["# Check the returned dataframe\n","proc_minutes_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XttsSyo4S25W"},"source":["# Check which row does not have rate\n","proc_minutes_df.loc[proc_minutes_df['rate'].isnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYzmNCcJS25X"},"source":["plt.figure(figsize=(10,5))\n","sns.distplot(proc_minutes_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuBk9Jm0S25X"},"source":["# Check which row does not have rate\n","proc_minutes_df.loc[proc_minutes_df['rate'].isnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFjxaB7sS25X"},"source":["Meeting minutes includes various topics, so it makes sense to extract relevant sections. <br />\n","Check some common sections."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"HnXYejMiS25X"},"source":["# Check what kind of sections are in common\n","\n","a = minutes_df.contents.str.lower().str.count('staff economic outlook')\n","b = minutes_df.contents.str.lower().str.count('developments in financial markets')\n","c = minutes_df.contents.str.lower().str.count('staff review of the economic situation')\n","d = minutes_df.contents.str.lower().str.count('staff review of the financial situation')\n","e = minutes_df.contents.str.lower().str.count('participants\\' views on current condition')\n","f = minutes_df.contents.str.lower().str.count('committee policy action')\n","g = minutes_df.contents.str.lower().str.count('voting for this action')\n","h = minutes_df.contents.str.lower().str.count('federal fund')\n","\n","pd.options.display.max_rows = 300\n","pd.DataFrame({'date': minutes_df['date'], 'a': a, 'b':b, 'c':c, 'd':d, 'e':e, 'f':f, 'g':g, 'h':h})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNQmEH5XS25X"},"source":["pd.options.display.max_rows = 20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tt8nDAypS25Y"},"source":["Sections are added from time to time. For example, quarterly economic forcast started in 2009. Leave manually handling those for now, take another approach to filter sentenses by length and key words here."]},{"cell_type":"code","metadata":{"id":"uTBg5hT5S25Y"},"source":["proc_minutes_df = remove_short_section(proc_minutes_df, min_words=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zvGspT1S25Y"},"source":["### Split contents to max 200 words"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"uebCQujSS25Y"},"source":["split_minutes_df = get_split_df(proc_minutes_df)\n","print(split_minutes_df.shape)\n","split_minutes_df.tail(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzpy2GrcS25Y"},"source":["### Filter out by keywords"]},{"cell_type":"code","metadata":{"id":"QyEg7wmDS25Y"},"source":["# Keep sections having keywords and long enough\n","keyword_minutes_df = remove_short_nokeyword(proc_minutes_df)\n","keyword_minutes_df.reset_index(drop=True, inplace=True)\n","print(keyword_minutes_df.shape)\n","keyword_minutes_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrtSuEFES25Z"},"source":["plt.figure(figsize=(10,5))\n","sns.distplot(proc_minutes_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZ7LMSJrS25Z"},"source":["# Drop text during the process to reduce the output size\n","proc_minutes_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","split_minutes_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","keyword_minutes_df.drop(columns=['text_sections', 'org_text'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7dxF8SNVS25Z"},"source":["## Press Conference Script DataFrame\n","Press Conference transcripts are available only from 2011"]},{"cell_type":"code","metadata":{"id":"WJVyyedGS25Z"},"source":["# Sample\n","print(presconf_script_df['contents'][2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"T3LbHZ7HS25Z"},"source":["# Reorganize the dataframe\n","proc_presconf_script_df = reorganize_df(presconf_script_df, 'presconf_script')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDleuPHPS25Z"},"source":["# Check the returned dataframe\n","proc_presconf_script_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HphvjHcS25a"},"source":["# Check which row does not have rate\n","proc_presconf_script_df.loc[proc_presconf_script_df['rate'].isnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zv6fRzHaS25a"},"source":["### Split by speakers\n","Scripts contain words from different people, so split by the speaker"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"mXY5nadRS25a"},"source":["script_data_list = []\n","\n","for i, row in tqdm(proc_presconf_script_df.iterrows()):\n","    for text in row[\"text_sections\"]:\n","        match = re.findall(r'(^[A-Za-zŞ. ]*[A-Z]{3}).\\d? (.*)', text)\n","        if len(match) == 0:\n","            match = re.findall(r'(^[A-Za-zŞ. ]*[A-Z]{3}).\\d(.*)', text)\n","            if len(match) == 0:\n","                print(\"not matched: \", text)\n","                print(row['date'])\n","                print()\n","        if len(match) == 1:\n","            speaker, text = match[0]\n","            row['speaker'] = speaker\n","            row['text'] = text\n","            row['word_count'] = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text))\n","            script_data_list.append(list(row))\n","\n","col_name = proc_presconf_script_df.columns\n","\n","presconf_script_speaker_df = pd.DataFrame(script_data_list, columns = col_name)\n","presconf_script_speaker_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fw6iT7KyS25g"},"source":["# Filter by the word count\n","presconf_script_speaker_df = presconf_script_speaker_df.loc[presconf_script_speaker_df['word_count'] >= 50]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9Nwbzn1S25g"},"source":["print(presconf_script_speaker_df.shape)\n","sns.distplot(presconf_script_speaker_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Umb86TT8S25g"},"source":["# Filter to have only chairperson's speak\n","\n","tmp_list = []\n","for i, row in presconf_script_speaker_df.iterrows():\n","    chairperson = get_chairperson(row['date'])\n","    if chairperson.lower().split()[-1] in row['speaker'].lower():\n","        row['speaker'] = chairperson\n","        tmp_list.append(list(row))\n","\n","col_names = presconf_script_speaker_df.columns\n","presconf_script_chair_df = pd.DataFrame(data=tmp_list, columns=col_names)\n","presconf_script_chair_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"5t2FLeRpS25h"},"source":["# Combine scripts of the same speaker for the same day.\n","tmp_date = ''\n","tmp_speaker = ''\n","tmp_data = []\n","\n","print('Before: ', presconf_script_chair_df.shape)\n","\n","for i, row in tqdm(presconf_script_chair_df.iterrows()):\n","    if (row['date'] == tmp_date) and (row['speaker'] == tmp_speaker):\n","        tmp_data[-1]['text'] += row['text']\n","        tmp_data[-1]['word_count'] += row['word_count']\n","        tmp_data[-1]['text_sections'].append(row['text'])\n","    else:\n","        tmp_date = row['date']\n","        tmp_speaker = row['speaker']\n","        row['text_sections'] = [row['text']]\n","        tmp_data.append(row)\n","\n","presconf_script_chair_day_df = pd.DataFrame(tmp_data)\n","\n","print('After', presconf_script_chair_day_df.shape)\n","presconf_script_chair_day_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yN5RVft5S25h"},"source":["sns.distplot(list(presconf_script_chair_day_df['word_count']), bins=50, kde=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2Bv3l8QS25h"},"source":["# # Check text\n","# print(presconf_script_chair_day_df['text'][0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwjqpTx6S25h"},"source":["### Split contents to max 200 words"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"23mYjLJRS25h"},"source":["presconf_script_split_df = get_split_df(presconf_script_chair_day_df)\n","print(presconf_script_split_df.shape)\n","presconf_script_split_df.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JqPH8cGVS25i"},"source":["### Filter out by keywords"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"O8F2FwygS25i"},"source":["# Keep sections having keywords and long enough\n","presconf_script_keyword_df = remove_short_nokeyword(presconf_script_chair_day_df)\n","presconf_script_keyword_df.reset_index(drop=True, inplace=True)\n","print(presconf_script_keyword_df.shape)\n","presconf_script_keyword_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UY6YlYstS25i"},"source":["sns.distplot(list(presconf_script_keyword_df['word_count']), bins=50, kde=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVwuU600S25j"},"source":["# Drop text during the process to reduce the output size\n","presconf_script_chair_day_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","presconf_script_split_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","presconf_script_keyword_df.drop(columns=['text_sections', 'org_text'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4FS7GKPXS25j"},"source":["## Meeting Script DataFrame"]},{"cell_type":"code","metadata":{"id":"y_H7kcM9S25j"},"source":["# Sample\n","print(meeting_script_df['contents'][2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBGu86mdS25j"},"source":["# Reorganize the dataframe\n","proc_meeting_script_df = reorganize_df(meeting_script_df, 'meeting_script')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1DiVCXMS25j"},"source":["# Check the returned dataframe\n","proc_meeting_script_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B75jG3iUS25j"},"source":["# Check which row does not have rate\n","proc_meeting_script_df.loc[proc_meeting_script_df['rate'].isnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAb-7KM5S25k"},"source":["print(proc_meeting_script_df.shape)\n","sns.distplot(proc_meeting_script_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TXr3SllLS25k"},"source":["### Split by speakers\n","Scripts contain words from different people, so split by the speaker"]},{"cell_type":"code","metadata":{"id":"h3TSi4uMS25k"},"source":["script_data_list = []\n","\n","for i, row in tqdm(proc_meeting_script_df.iterrows()):\n","    for text in row[\"text_sections\"]:\n","        match = re.findall(r'(^[A-Za-zŞ. ]*[A-Z]{3}).\\d? (.*)', text)\n","        if len(match) == 0:\n","            match = re.findall(r'(^[A-Za-zŞ. ]*[A-Z]{3}).\\d(.*)', text)\n","            if len(match) == 0:\n","                print(\"not matched: \", text)\n","                print(row['date'])\n","                print()\n","        if len(match) == 1:\n","            speaker, text = match[0]\n","            row['speaker'] = speaker\n","            row['text'] = text\n","            row['word_count'] = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text))\n","            script_data_list.append(list(row))\n","\n","col_name = proc_meeting_script_df.columns\n","\n","meeting_script_speaker_df = pd.DataFrame(script_data_list, columns = col_name)\n","meeting_script_speaker_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eKzloBfwS25k"},"source":["Some are not matching the defined patterns but those unmatched ones do not look importnant. Thus ignore them here."]},{"cell_type":"code","metadata":{"id":"FGEyuPYmS25l"},"source":["# Filter by word count having 20 or more.\n","print(\"Before: \", meeting_script_speaker_df.shape)\n","meeting_script_speaker_df = meeting_script_speaker_df.loc[meeting_script_speaker_df['word_count'] >= 20]\n","print(\"After: \", meeting_script_speaker_df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRUuiNY6S25l"},"source":["sns.distplot(meeting_script_speaker_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbnkJZhFS25l"},"source":["# Combine scripts for the same day.\n","meeting_script_speaker_df = meeting_script_speaker_df.groupby(['type', 'date', 'title', 'speaker', 'decision', 'rate', 'next_meeting', 'next_decision', 'next_rate'])['text'].apply('[SECTION]'.join).reset_index()\n","meeting_script_speaker_df['text_sections'] = meeting_script_speaker_df['text'].map(lambda x: x.split(\"[SECTION]\"))\n","meeting_script_speaker_df['text'] = meeting_script_speaker_df['text'].map(lambda x: x.replace(\"[SECTION]\", \"\"))\n","meeting_script_speaker_df['word_count'] = meeting_script_speaker_df['text'].map(get_word_count)\n","meeting_script_speaker_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tfvFhjnS25l"},"source":["sns.distplot(meeting_script_speaker_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eo1LJVqFS25l"},"source":["### Split contents to max 200 words"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"GdnbCoe4S25l"},"source":["meeting_script_split_df = get_split_df(meeting_script_speaker_df)\n","print(meeting_script_split_df.shape)\n","meeting_script_split_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3PlXV-p6S25m"},"source":["### Filter out by keywords"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"CGyQh7mbS25m"},"source":["# Keep sections having keywords and long enough\n","meeting_script_keyword_df = remove_short_nokeyword(meeting_script_speaker_df)\n","meeting_script_keyword_df.reset_index(drop=True, inplace=True)\n","print(meeting_script_keyword_df.shape)\n","meeting_script_keyword_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLQsU4d7S25m"},"source":["# Drop text during the process to reduce the output size\n","meeting_script_speaker_df.drop(columns=['text_sections'], inplace=True)\n","meeting_script_split_df.drop(columns=['text_sections'], inplace=True)\n","meeting_script_keyword_df.drop(columns=['text_sections'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtfovSwyS25m"},"source":["## Speech DataFrame"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"xLUbs717S25m"},"source":["# Reorganize the dataframe\n","proc_speech_df = reorganize_df(speech_df, 'speech')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0liYkViMS25m"},"source":["# Check the returned dataframe\n","# Note that rate and decision are not applicable because speech is not at FOMC meeting\n","proc_speech_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"fyFQJRzyS25n"},"source":["print(proc_speech_df.shape)\n","sns.distplot(proc_speech_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"afxwIHVHS25n"},"source":["# Check records where speaker is null\n","proc_speech_df['speaker'].isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQwLEWUgS25n"},"source":["# Remove sections having less than 50 words because those are unlikely to contain meaningful sentences.\n","proc_speech_df = remove_short_section(proc_speech_df, min_words=50)\n","proc_speech_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SNrXsKsS25n"},"source":["# Extract Chairperson's speech\n","tmp_list = []\n","for i, row in proc_speech_df.iterrows():\n","    chairperson = get_chairperson(row['date'])\n","    if chairperson.lower().split()[-1] in row['speaker'].lower():\n","        row['speaker'] = chairperson\n","        tmp_list.append(list(row))\n","\n","col_names = proc_speech_df.columns\n","speech_chair_df = pd.DataFrame(data=tmp_list, columns=col_names)\n","\n","print(speech_chair_df.shape)\n","speech_chair_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vtbZaG6OS25o"},"source":["### Split contents to max 200 words"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"xHi5WG7QS25o"},"source":["speech_split_df = get_split_df(speech_chair_df)\n","speech_split_df.reset_index(drop=True, inplace=True)\n","print(speech_split_df.shape)\n","speech_split_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwzLVSk6S25o"},"source":["### Filter out by keyword"]},{"cell_type":"code","metadata":{"id":"FmwsVWTUS25o"},"source":["# Keep sections having keywords and long enough\n","speech_keyword_df = remove_short_nokeyword(speech_chair_df)\n","speech_keyword_df.reset_index(drop=True, inplace=True)\n","print(speech_keyword_df.shape)\n","speech_keyword_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxxEJnHYS25o"},"source":["# Drop text_sections\n","speech_chair_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","speech_split_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","speech_keyword_df.drop(columns=['text_sections', 'org_text'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJiodECQS25o"},"source":["## Testimony DataFrame"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"1UtVvEI6S25p"},"source":["# Reorganize the dataframe\n","# Note that rate and decision are not applicable because testimony is not at FOMC meeting\n","proc_testimony_df = reorganize_df(testimony_df, 'testimony')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljhWQry6S25p"},"source":["# Check the returned dataframe\n","proc_testimony_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhC8sd-iS25p"},"source":["print(proc_testimony_df.shape)\n","sns.distplot(proc_testimony_df[\"word_count\"].values, bins=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3KtyfyxVS25p"},"source":["# Check records where speaker is null\n","proc_testimony_df['speaker'].isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcyRl7w9S25p"},"source":["# Remove sections having less than 50 words because those are unlikely to contain meaningful sentences.\n","proc_testimony_df = remove_short_section(proc_testimony_df, min_words=50)\n","proc_testimony_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4j5j8BDPS25q"},"source":["# Extract Chairperson's speech\n","tmp_list = []\n","for i, row in proc_testimony_df.iterrows():\n","    chairperson = get_chairperson(row['date'])\n","    if chairperson.lower().split()[-1] in row['speaker'].lower():\n","        row['speaker'] = chairperson\n","        tmp_list.append(list(row))\n","\n","col_names = proc_testimony_df.columns\n","testimony_chair_df = pd.DataFrame(data=tmp_list, columns=col_names)\n","testimony_chair_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQmfLPBGS25q"},"source":["### Split contents to max 200 words"]},{"cell_type":"code","metadata":{"id":"TynSZrpaS25q"},"source":["testimony_split_df = get_split_df(testimony_chair_df)\n","testimony_split_df.reset_index(drop=True, inplace=True)\n","print(testimony_split_df.shape)\n","testimony_split_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"25IYZ6vPS25q"},"source":["### Filter out by keyword"]},{"cell_type":"code","metadata":{"id":"gS5954guS25q"},"source":["# Keep sections having keywords and long enough\n","testimony_keyword_df = remove_short_nokeyword(testimony_chair_df)\n","testimony_keyword_df.reset_index(drop=True, inplace=True)\n","print(testimony_keyword_df.shape)\n","testimony_keyword_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_jNh9C4S25q"},"source":["# Drop text sections\n","testimony_chair_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","testimony_split_df.drop(columns=['text_sections', 'org_text'], inplace=True)\n","testimony_keyword_df.drop(columns=['text_sections', 'org_text'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"judPbphDS25r"},"source":["## Save the train data"]},{"cell_type":"code","metadata":{"id":"7oI8BxriS25r"},"source":["# Combine dataframes\n","text_no_split = pd.concat([proc_statement_df, \n","                           proc_minutes_df, \n","                           presconf_script_chair_day_df, \n","                           meeting_script_speaker_df, \n","                           speech_chair_df,\n","                           testimony_chair_df], sort=False)\n","text_no_split.reset_index(drop=True, inplace=True)\n","\n","text_split_200 = pd.concat([split_statement_df, \n","                            split_minutes_df, \n","                            presconf_script_split_df, \n","                            meeting_script_split_df, \n","                            speech_split_df, \n","                            testimony_split_df], sort=False)\n","text_split_200.reset_index(drop=True, inplace=True)\n","\n","text_keyword = pd.concat([keyword_statement_df,\n","                          keyword_minutes_df,\n","                          presconf_script_keyword_df,\n","                          meeting_script_keyword_df, \n","                          speech_keyword_df, \n","                          testimony_keyword_df], sort=False)\n","text_keyword.reset_index(drop=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BL6WS5XS25r"},"source":["def save_data(df, file_name, dir_name='../data/preprocessed/'):\n","    '''\n","    Save the given df to pickle file and csv file in the given directory.\n","    '''\n","    if not os.path.exists(dir_name):\n","        os.mkdir(dir_name)\n","    \n","    # Save results to a picke file\n","    file = open(dir_name + file_name + '.pickle', 'wb')\n","    pickle.dump(df, file)\n","    file.close()\n","    print(\"Data Saved to a pickle file in {} !\".format(dir_name))\n","\n","    # Save results to a csv file\n","    df.to_csv(dir_name + file_name + '.csv', index=True)\n","    print(\"Data Saved to a csv file in {} !\".format(dir_name))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wC-M2NtqS25r"},"source":["save_data(text_no_split, 'text_no_split')\n","save_data(text_split_200, 'text_split_200')\n","save_data(text_keyword, 'text_keyword')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cv6iZkgS25r"},"source":[""],"execution_count":null,"outputs":[]}]}