{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"6_FOMC_Corpora.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Kc6Xn7hAkC3V"},"source":["# Predicting interest rates from Federal Reserve documents\n","## Model Training (Vol. 6)\n","FE 690: Machine Learning in Finance \\\\\n","Author: Theo Dimitrasopoulos \\\\\n","Advisor: Zachary Feinstein \\\\"]},{"cell_type":"code","metadata":{"id":"X2MZzFxZw5pK"},"source":["import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","IN_COLAB"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ek5xugZfrjRb"},"source":["if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEvZT1hGw5pN"},"source":["# Use TPU\n","if IN_COLAB:\n","  # TPU Setting\n","  import os\n","  assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook setting > Hardware accelerator'  \n","  VERSION = \"20200220\"\n","  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","  !python pytorch-xla-env-setup.py --version $VERSION"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nEMWrknszFc"},"source":["# Uninstall existing versions:\n","#!pip uninstall numpy -y\n","#!pip uninstall pandas -y\n","#!pip uninstall tqdm -y\n","#!pip uninstall torch -y\n","#!pip uninstall scikit-plot -y\n","#!pip uninstall transformers -y\n","\n","# Install packages:\n","!pip install numpy==1.16.4\n","!pip install pandas==0.25.0\n","!pip install torch==1.4.0\n","!pip install tqdm==4.43.0\n","!pip install scikit-plot\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYiY1suXzjKg"},"source":["# Python libraries\n","import datetime as dt\n","import re\n","import pickle\n","from tqdm.notebook import tqdm\n","import time\n","import logging\n","import random\n","from collections import defaultdict, Counter\n","\n","# Data Science modules\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","plt.style.use('ggplot')\n","\n","# Import Scikit-learn moduels\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import model_selection\n","from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, StratifiedKFold, learning_curve, RandomizedSearchCV\n","import scikitplot as skplt\n","\n","# Import nltk modules and download dataset\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","stop = set(stopwords.words('english'))\n","\n","# Import Pytorch modules\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","from torch.autograd import Variable\n","from torch.optim import Adam, AdamW"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-37Bpnkw5pT"},"source":["# Set logger\n","logger = logging.getLogger('mylogger')\n","logger.setLevel(logging.INFO)\n","\n","timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n","formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n","\n","fh = logging.FileHandler('log_model.txt')\n","fh.setLevel(logging.DEBUG)\n","fh.setFormatter(formatter)\n","logger.addHandler(fh)\n","\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.INFO)\n","ch.setFormatter(formatter)\n","logger.addHandler(ch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYf9oi6fw5pV"},"source":["# Set Random Seed\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","rand_seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cb55v8tkw5pX"},"source":["# Set Seaborn Style\n","sns.set(style='white', context='notebook', palette='deep')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSVVc9rew5pd"},"source":["## Load preprocessed data"]},{"cell_type":"code","metadata":{"id":"sLFBUS8Hw5pf"},"source":["if IN_COLAB:\n","  employment_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/Employment/'\n","  cpi_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/CPI/'\n","  fed_rates_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/FEDRates/'\n","  fx_rates_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/FXRates/'\n","  gdp_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/GDP/'\n","  ism_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/ISM/'\n","  sales_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/Sales/'\n","  treasury_data_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/MarketData/Treasury/'\n","  fomc_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/FOMC/'\n","  preprocessed_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/preprocessed/'\n","  train_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/train_data/'\n","  output_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/result/'\n","  keyword_lm_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/LoughranMcDonald/'\n","  glove_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/GloVe/'\n","  model_dir = '/content/drive/My Drive/Colab Notebooks/proj2/src/data/models/'\n","else:\n","  employment_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/Employment/'\n","  cpi_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/CPI/'\n","  fed_rates_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/FEDRates/'\n","  fx_rates_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/FXRates/'\n","  gdp_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/GDP/'\n","  ism_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/ISM/'\n","  sales_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/Sales/'\n","  treasury_data_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/MarketData/Treasury/'\n","  fomc_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/FOMC/'\n","  preprocessed_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/preprocessed/'\n","  train_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/train_data/'\n","  output_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/result/'\n","  keyword_lm_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/LoughranMcDonald/'\n","  glove_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/GloVe/'\n","  model_dir = 'C:/Users/theon/GDrive/Colab Notebooks/proj2/src/data/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TC_3RwSGw5ph","scrolled":true},"source":["# Load previously processed non-text data\n","# Load data\n","file = open(train_dir + 'nontext_train_small.pickle', 'rb')\n","train_df = pickle.load(file)\n","file.close()\n","#train_df = pd.read_csv(train_dir + 'nontext_train_small.csv')\n","#print(train_df.shape)\n","#train_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wW8pWlJjw5pi"},"source":["# List of Non-text columns\n","nontext_columns = train_df.drop(columns=['target']).columns.tolist()\n","nontext_columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UgJAxAcw5pk"},"source":["# Load text data\n","file = open(preprocessed_dir + 'text_no_split.pickle', 'rb') # Original text\n","text_no_split = pickle.load(file)\n","file.close()\n","#text_no_split = pd.read_csv(preprocessed_dir + 'text_no_split.csv')\n","file = open(preprocessed_dir + 'text_split_200.pickle', 'rb') # Split at 200 words\n","text_split_200 = pickle.load(file)\n","file.close()\n","#text_split_200 = pd.read_csv(preprocessed_dir + 'text_split_200.csv')\n","file = open(preprocessed_dir + 'text_keyword.pickle', 'rb') # Paragraphs filtered for those having keywords\n","text_keyword = pickle.load(file)\n","file.close()\n","#text_keyword = pd.read_csv(preprocessed_dir + 'text_keyword.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_htY1Ljw5pm"},"source":["## Check the statistics of texts"]},{"cell_type":"code","metadata":{"id":"Vw7FJtqEw5pm"},"source":["# Check the number of records per document type\n","fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,7))\n","sns.countplot(x='type', data=text_no_split, ax=ax1)\n","ax1.set_title('text_no_split')\n","ax1.tick_params('x', labelrotation=45)\n","sns.countplot(x='type', data=text_split_200, ax=ax2)\n","ax2.set_title('text_split_200')\n","ax2.tick_params('x', labelrotation=45)\n","sns.countplot(x='type', data=text_keyword, ax=ax3)\n","ax3.set_title('text_keyword')\n","ax3.tick_params('x', labelrotation=45)\n","fig.suptitle(\"The nuber of records\", fontsize=16)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TwTiHBfpw5po"},"source":["text_no_split.loc[text_no_split['type'] == 'meeting_script'].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytNmriEhK14r"},"source":["# Check the number of records per document type\n","fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,7))\n","sns.countplot(x='type', data=text_no_split, ax=ax1)\n","ax1.set_title('text_no_split')\n","ax1.tick_params('x', labelrotation=45)\n","sns.countplot(x='type', data=text_split_200, ax=ax2)\n","ax2.set_title('text_split_200')\n","ax2.tick_params('x', labelrotation=45)\n","sns.countplot(x='type', data=text_keyword, ax=ax3)\n","ax3.set_title('text_keyword')\n","ax3.tick_params('x', labelrotation=45)\n","fig.suptitle(\"The nuber of records\", fontsize=16)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5bYkzD7K14u"},"source":["text_no_split.loc[text_no_split['type'] == 'meeting_script'].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OG09HqJaK6f1"},"source":["# Check the number of records per document type\n","fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,7))\n","sns.countplot(x='type', data=text_no_split, ax=ax1)\n","ax1.set_title('text_no_split')\n","ax1.tick_params('x', labelrotation=45)\n","sns.countplot(x='type', data=text_split_200, ax=ax2)\n","ax2.set_title('text_split_200')\n","ax2.tick_params('x', labelrotation=45)\n","sns.countplot(x='type', data=text_keyword, ax=ax3)\n","ax3.set_title('text_keyword')\n","ax3.tick_params('x', labelrotation=45)\n","fig.suptitle(\"The nuber of records\", fontsize=16)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ko3wWYtzw5px"},"source":["# Select one from the above different pre-processed data\n","text_df = text_no_split\n","text_df.reset_index(drop=True, inplace=True)\n","print(text_df.shape)\n","text_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3MOs7yZKdFz","scrolled":false},"source":["# Check distribution\n","\n","fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(15,15))\n","doc_type = 'statement'\n","sns.distplot(text_df.loc[text_df['type'] == doc_type]['word_count'], bins=20, ax=ax1, kde=False, color='r')\n","ax1.set_title(doc_type)\n","doc_type = 'minutes'\n","sns.distplot(text_df.loc[text_df['type'] == doc_type]['word_count'], bins=20, ax=ax2, kde=False, color='y')\n","ax2.set_title(doc_type)\n","doc_type = 'presconf_script'\n","sns.distplot(text_df.loc[text_df['type'] == doc_type]['word_count'], bins=20, ax=ax3, kde=False, color='g')\n","ax3.set_title(doc_type)\n","doc_type = 'speech'\n","sns.distplot(text_df.loc[text_df['type'] == doc_type]['word_count'], bins=20, ax=ax4, kde=False, color='b')\n","ax4.set_title(doc_type)\n","doc_type = 'testimony'\n","sns.distplot(text_df.loc[text_df['type'] == doc_type]['word_count'], bins=20, ax=ax5, kde=False, color='purple')\n","ax5.set_title(doc_type)\n","\n","fig.tight_layout(pad=3.0)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AeyWwu_w5p0"},"source":["# Check balance of Rate Decision\n","\n","#g = sns.FacetGrid(text_df, col='type', height=6, aspect=0.5)\n","#g.map(sns.countplot, 'next_decision')\n","#plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8xBjmHWw5p2"},"source":["from collections import defaultdict \n","\n","doc_types = text_df['type'].unique()\n","\n","merged_dict = defaultdict(list)\n","\n","for i, row in train_df.iterrows():\n","    text_rows = text_df.loc[text_df['next_meeting'] == i]\n","    merged_text_all = \"\"\n","    for doc_type in doc_types:\n","        merged_text = \"\"\n","        for text in text_rows.loc[text_rows['type'] == doc_type]['text']:\n","            merged_text += \" \" + text\n","        merged_dict[doc_type].append(merged_text)\n","        merged_text_all += merged_text\n","    merged_dict['text'].append(merged_text_all)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p23iGko3w5p4"},"source":["for key in merged_dict.keys():\n","    train_df[key] = merged_dict[key]\n","\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kq554uOHLIJ8"},"source":["from collections import defaultdict \n","\n","doc_types = text_df['type'].unique()\n","\n","merged_dict = defaultdict(list)\n","\n","for i, row in train_df.iterrows():\n","    text_rows = text_df.loc[text_df['next_meeting'] == i]\n","    merged_text_all = \"\"\n","    for doc_type in doc_types:\n","        merged_text = \"\"\n","        for text in text_rows.loc[text_rows['type'] == doc_type]['text']:\n","            merged_text += \" \" + text\n","        merged_dict[doc_type].append(merged_text)\n","        merged_text_all += merged_text\n","    merged_dict['text'].append(merged_text_all)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeTNjN72LIJ-"},"source":["for key in merged_dict.keys():\n","    train_df[key] = merged_dict[key]\n","\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgiFdMPXLIKA"},"source":["# Check if most of docs are merged\n","\n","count_text, count_train = 0, 0\n","\n","for doc_type in doc_types:\n","    count = 0\n","    for text in text_df.loc[text_df['type']==doc_type]['text']:\n","        count += len(text.split())\n","    print(\"{} words in original text for {}\".format(count, doc_type))\n","    count_text += count\n","    \n","    count = 0\n","    for text in train_df[doc_type]:\n","        count += len(text.split())\n","    print(\"{} words in merged text for {}\".format(count, doc_type))\n","    count_train += count\n","\n","print(\"Total: {} words in original text\".format(count_text))\n","print(\"Total: {} words in merged text\".format(count_train))\n","print(\"Total: {} words in text column of merged text\".format(train_df['text'].apply(lambda x: len(x.split())).sum()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQ7Wj1URLeiA"},"source":["#print(\"Before dropping: \", train_df.shape)\n","#train_df = train_df.loc[train_df['text'] != \"\"]\n","#print(\"After dropping: \", train_df.shape)\n","#train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2VbwpfWmKFqC"},"source":["# Corpus\n","def create_corpus(df):\n","    corpus = []\n","    \n","    for x in df['text'].str.split():\n","        for i in x:\n","            corpus.append(i.lower())\n","    return corpus\n","\n","# Returns Top X frequent stop words\n","def get_frequent_stop_words(corpus, top_n=10):\n","    dic = defaultdict(int)\n","    for word in corpus:\n","        if word in stop:\n","            dic[word] += 1\n","\n","    top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:top_n]\n","\n","    return zip(*top)\n","\n","# Returns Top X frequent non stop words\n","def get_frequent_nonstop_words(corpus, top_n=10):\n","    dic = defaultdict(int)\n","    for word in corpus:\n","        if word not in stop:\n","            dic[word] += 1\n","\n","    top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:top_n]\n","\n","    return zip(*top)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xPeJBe5w5p_"},"source":["corpus = create_corpus(text_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYHFFbWcw5qB"},"source":["x, y = get_frequent_stop_words(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_A6UBhOw5qC"},"source":["print(x)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdIxqD7ow5qE"},"source":["x, y = get_frequent_nonstop_words(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tu3_wU1Yw5qG"},"source":["print(x)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6FPeC8DKWm6"},"source":["# Check most frequent words which are not in stopwords\n","counter = Counter(corpus)\n","most = counter.most_common()[:60]\n","x, y = [], []\n","for word, count in most:\n","    if word not in stop:\n","        x.append(word)\n","        y.append(count)\n","\n","plt.figure(figsize=(15,7))\n","sns.barplot(x=y, y=x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PluR4k7J9pv"},"source":["# Generate Word Cloud image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","# Create stopword list:\n","stopwords = set(STOPWORDS)\n","stopwords.update([\"federal\", \"federal reserve\", \"financial\", \"committee\", \"market\", \"would\", \"also\"])\n","\n","text = \" \".join(corpus)\n","\n","# Generate a word cloud image\n","wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n","plt.figure(figsize=(15,7))\n","# Display the generated image:\n","# the matplotlib way:\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()\n","\n","# Generate a word cloud image\n","wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjIknnQUw5qJ","scrolled":true},"source":["# Load sentiment data\n","sentiment_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/proj2/src/data/LoughranMcDonald/LoughranMcDonald_SentimentWordLists_2018.csv')\n","print(sentiment_df.shape)\n","sentiment_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-dsq8Ngw5qL"},"source":["# Make all words lower case\n","sentiment_df['word'] = sentiment_df['word'].str.lower()\n","sentiments = sentiment_df['sentiment'].unique()\n","sentiment_df.groupby(by=['sentiment']).count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBhjXmUTw5qM"},"source":["sentiment_dict = { sentiment: sentiment_df.loc[sentiment_df['sentiment']==sentiment]['word'].values.tolist() for sentiment in sentiments}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fcVciwegw5qP"},"source":["## Analyze the tone with negation without lemmatization"]},{"cell_type":"code","metadata":{"id":"PskYKa4aw5qP"},"source":["# Consider Negation\n","negate = [\"cannot\", \"ain't\", \"aren't\", \"can't\",\n","          \"couldn't\", \"daren't\", \"didn't\", \"doesn't\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\",\n","          \"neither\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"neednt\", \"needn't\",\n","          \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\", \"oughtnt\", \"shant\", \"shouldnt\", \"wasnt\",\n","          \"werent\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"without\", \"wont\", \"wouldnt\", \"won't\",\n","          \"wouldn't\", \"rarely\", \"seldom\", \"despite\", \"no\", \"nobody\"]\n","\n","def negated(word):\n","    \"\"\"\n","    Determine if preceding word is a negation word\n","    \"\"\"\n","    if word.lower() in negate:\n","        return True\n","    else:\n","        return False\n","\n","def tone_count_with_negation_check(dict, article):\n","    \"\"\"\n","    Count positive and negative words with negation check. Account for simple negation only for positive words.\n","    Simple negation is taken to be observations of one of negate words occurring within three words\n","    preceding a positive words.\n","    \"\"\"\n","    pos_count = 0\n","    neg_count = 0\n","    tone_score = 0\n","\n","    pos_words = []\n","    neg_words = []\n"," \n","    input_words = re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', article.lower())\n","    word_count = len(input_words)\n","     \n","    for i in range(0, word_count):\n","        if input_words[i] in dict['Negative']:\n","            neg_count += 1\n","            neg_words.append(input_words[i])\n","        if input_words[i] in dict['Positive']:\n","            if i >= 3:\n","                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n","                    neg_count += 1\n","                    neg_words.append(input_words[i] + ' (with negation)')\n","                else:\n","                    pos_count += 1\n","                    pos_words.append(input_words[i])\n","            elif i == 2:\n","                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n","                    neg_count += 1\n","                    neg_words.append(input_words[i] + ' (with negation)')\n","                else:\n","                    pos_count += 1\n","                    pos_words.append(input_words[i])\n","            elif i == 1:\n","                if negated(input_words[i - 1]):\n","                    neg_count += 1\n","                    neg_words.append(input_words[i] + ' (with negation)')\n","                else:\n","                    pos_count += 1\n","                    pos_words.append(input_words[i])\n","            elif i == 0:\n","                pos_count += 1\n","                pos_words.append(input_words[i])\n"," \n","    if word_count > 0:\n","        tone_score = 100 * (pos_count - neg_count) / word_count\n","    else:\n","        tone_score = 0\n","    \n","    results = [tone_score, word_count, pos_count, neg_count, pos_words, neg_words]\n"," \n","    return results\n","\n","columns = ['tone_score', 'word_count', 'n_pos_words', 'n_neg_words', 'pos_words', 'neg_words']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NjuD9SWw5qR"},"source":["# Analyze tone for original text dataframe\n","print(text_df.shape)\n","tone_keyword_lm = [tone_count_with_negation_check(sentiment_dict, x) for x in tqdm(text_df['text'], total=text_df.shape[0])]\n","tone_keyword_lm_df = pd.DataFrame(tone_keyword_lm, columns=columns)\n","text_df = pd.concat([text_df, tone_keyword_lm_df.reindex(text_df.index)], axis=1)\n","text_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20abWG1zw5qT"},"source":["# Analyze tone for training dataframe\n","tone_lmdict_list = []\n","for doc_type in doc_types:\n","    tone_lmdict = [tone_count_with_negation_check(sentiment_dict, x)[0] for x in tqdm(train_df[doc_type], \n","                                                                                      total=train_df.shape[0], \n","                                                                                      desc=doc_type)]\n","    tone_lmdict_list.append(tone_lmdict)\n","    \n","train_df['tone'] = np.mean(tone_lmdict_list, axis=0)\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSp8OS1ow5qV"},"source":["train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Adkv3IXw5qX"},"source":["# Show corelations to next_decision\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n","corr_columns = ['target', 'tone', 'prev_decision']\n","sns.heatmap(train_df[corr_columns].astype(float).corr(), cmap=\"YlGnBu\", annot=True, fmt=\".2f\", ax=ax1, vmin=0, vmax=1)\n","ax1.set_title(\"Correlation of train_df\")\n","corr_columns = ['next_decision', 'tone_score', 'decision']\n","sns.heatmap(text_df[corr_columns].astype(float).corr(), cmap=\"YlGnBu\", annot=True, fmt=\".2f\", ax=ax2, vmin=0, vmax=1)\n","ax2.set_title(\"Correlation of text_df\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uiYJ4EOFw5qb"},"source":["# Per document type\n","corr_columns = ['next_decision', 'tone_score', 'type']\n","doc_types = ['statement', 'minutes', 'presconf_script', 'meeting_script', 'speech', 'testimony']\n","\n","fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15,7))\n","\n","axes = [ax1, ax2, ax3, ax4, ax5, ax6]\n","df = text_df[corr_columns]\n","for i, doc_type in enumerate(doc_types):\n","    sns.heatmap(df.loc[df['type'] == doc_type].drop(columns=['type']).astype(float).corr(), cmap=\"YlGnBu\", annot=True, fmt=\".2f\", vmin=0, vmax=1, ax=axes[i])\n","    axes[i].set_title(doc_type)\n","\n","fig.suptitle('Correlation of text_df', fontsize=24)\n","fig.tight_layout(pad=3.0)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mDyuSbyw5qd"},"source":["def lemmatize_word(word):\n","    wnl = nltk.stem.WordNetLemmatizer()\n","    return wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v')\n","\n","def tokenize_df(df, col='text'):\n","    tokenized = []\n","    wnl = nltk.stem.WordNetLemmatizer()\n","    for text in tqdm(df[col]):\n","        # Filter alphabet words only and non stop words, make it loser case\n","        words = [word.lower() for word in word_tokenize(text) if ((word.isalpha()==1) & (word not in stop))]\n","        # Lemmatize words \n","        tokens = [lemmatize_word(word) for word in words]\n","        tokenized.append(tokens)\n","    return tokenized\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDxDFgxew5qh"},"source":["tokenized_org = tokenize_df(text_df)\n","print('len(tokenized_org): ', len(tokenized_org))\n","print(tokenized_org[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5nKnuYuw5qi"},"source":["# Concat the list to create docs\n","lemma_docs_org = [\" \".join(words) for words in tokenized_org]\n","print('len(lemma_docs_org): ', len(lemma_docs_org))\n","print(lemma_docs_org[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6qUaw1Lw5qk"},"source":["# Create a list of all the words in the dataframe\n","all_words_org = [word for text in tokenized_org for word in text]\n","print('len(all_words_org): ', len(all_words_org))\n","print(all_words_org[0])\n","\n","# Counter object of all the words\n","counts_org = Counter(all_words_org)\n","print('len(counts_org): ', len(counts_org))\n","\n","# Create a Bag of Word, sorted by the count of words\n","bow_org = sorted(counts_org, key=counts_org.get, reverse=True)\n","print('bow_org[:20]', bow_org[:20])\n","\n","# Indexing vocabrary, starting from 1.\n","vocab_org = {word: ii for ii, word in enumerate(counts_org, 1)}\n","id2vocab_org = {v: k for k, v in vocab_org.items()}\n","\n","print(\"vocab_org['chairman']: \", vocab_org['chairman'])\n","print(\"vocab_org['market']: \", vocab_org['market'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uTFjFJ1ow5qm"},"source":["# Create token id list\n","token_ids_org = [[vocab_org[word] for word in text_words] for text_words in tokenized_org]\n","print(len(token_ids_org))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32mKXiA8w5qo"},"source":["# Add to the dataframe\n","text_df['tokenized'] = tokenized_org\n","text_df['token_ids'] = token_ids_org"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-aLb0Upw5qp"},"source":["# #Â Filter by frequency of words\n","# # This time, switch it off as the frequency is already considered while creating the vocabrary\n","\n","freq = {}\n","num_words = len(all_words)\n","print('len(all_words): ', len(all_words))\n","\n","for key in counts:\n","  freq[key] = counts[key]/num_words\n","\n","print('len(freq): ', len(freq))\n","print(freq['rate'])\n","\n","low_cutoff = 0.000001\n","high_cutoff = 20\n","\n","K_most_common, K_most_common_values = zip(*counts.most_common()[:high_cutoff])\n","filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n","print(K_most_common)\n","print('len(filtered_words): ', len(filtered_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXHFi117w5qq"},"source":["tokenized = tokenize_df(train_df)\n","print('len(tokenized): ', len(tokenized))\n","print(tokenized[0])\n","\n","# Concat the list to create docs\n","lemma_docs = [\" \".join(words) for words in tokenized]\n","print('len(lemma_docs): ', len(lemma_docs))\n","print(lemma_docs[0])\n","\n","# Create a list of all the words in the dataframe\n","all_words = [word for text in tokenized for word in text]\n","print('len(all_words): ', len(all_words))\n","print(all_words[0])\n","\n","# Counter object of all the words\n","counts = Counter(all_words)\n","print('len(counts): ', len(counts))\n","\n","# Create a Bag of Word, sorted by the count of words\n","bow = sorted(counts, key=counts.get, reverse=True)\n","print('bow[:20]', bow[:20])\n","\n","# Indexing vocabrary, starting from 1.\n","vocab = {word: ii for ii, word in enumerate(counts, 1)}\n","id2vocab = {v: k for k, v in vocab.items()}\n","\n","# Create token id list\n","token_ids = [[vocab[word] for word in text_words] for text_words in tokenized]\n","print(len(token_ids))\n","\n","# Add to the dataframe\n","train_df['tokenized'] = tokenized\n","train_df['token_ids'] = token_ids\n","train_df['tokenized_text'] = train_df['tokenized'].apply(lambda x: \" \".join(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEZNjFOIw5qs"},"source":["sns.distplot(train_df['tokenized_text'].apply(lambda x: len(x.split())))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0SANAFyw5qu"},"source":["len(token_ids[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ri2gztVOw5qw"},"source":["## Lemmatize sentiment"]},{"cell_type":"code","metadata":{"id":"YRaYtgpDw5qw"},"source":["# pd.get_dummies(sentiment_df, prefix=None, dtype=bool)\n","# sentiment_df.columns = [column.lower() for column in sentiment_df.columns]\n","\n","# Lemmertize sentiment words as well\n","lemma_sentiment_df = sentiment_df.copy(deep=True)\n","lemma_sentiment_df['word'] = [lemmatize_word(word) for word in lemma_sentiment_df['word']]\n","# Drop duplicates\n","lemma_sentiment_df = sentiment_df.drop_duplicates('word')\n","# Sentiment list\n","lemma_sentiments = list(lemma_sentiment_df['sentiment'].unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tu1eUGOaw5qz"},"source":["lemma_sentiment_df.groupby(by=['sentiment']).count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzJ80-9jw5q0"},"source":["## Tfidf "]},{"cell_type":"code","metadata":{"id":"kJrDqA9gw5q0"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def get_tfidf(sentiment_words, docs):\n","    \"\"\"\n","    Generate TFIDF values from documents for a certain sentiment\n","\n","    Parameters\n","    ----------\n","    sentiment_words: Pandas Series\n","        Words that signify a certain sentiment\n","    docs : list of str\n","        List of documents used to generate bag of words\n","\n","    Returns\n","    -------\n","    tfidf : 2-d Numpy Ndarray of float\n","        TFIDF sentiment for each document\n","        The first dimension is the document.\n","        The second dimension is the word.\n","    \"\"\"\n","    vectorizer = TfidfVectorizer(analyzer='word', vocabulary=sentiment_words)\n","    tfidf = vectorizer.fit_transform(docs)\n","    features = vectorizer.get_feature_names()\n","    \n","    return tfidf.toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l6oIPhgTw5q2"},"source":["### Text dataframe"]},{"cell_type":"code","metadata":{"id":"F2TsPhW0w5q3"},"source":["# Using the get_tfidf function, let's generate the TFIDF values for all the documents.\n","sentiment_tfidf_org = {\n","        sentiment: get_tfidf(lemma_sentiment_df.loc[lemma_sentiment_df['sentiment'] == sentiment]['word'], lemma_docs_org)\n","        for sentiment in lemma_sentiments}\n","\n","print(len(sentiment_tfidf_org['Negative']))\n","print(len(sentiment_tfidf_org['Negative'][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OhSLtjmaw5q4"},"source":["text_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awD1-BEPw5q6"},"source":["for sentiment in lemma_sentiments:\n","    text_df['tfidf_' + sentiment] = list(sentiment_tfidf_org[sentiment])\n","    \n","text_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXzXpqynw5q7"},"source":["### Train dataframe"]},{"cell_type":"code","metadata":{"id":"vL2TZFLOw5q8"},"source":["# Using the get_tfidf function, let's generate the TFIDF values for all the documents.\n","sentiment_tfidf = {\n","        sentiment: get_tfidf(lemma_sentiment_df.loc[lemma_sentiment_df['sentiment'] == sentiment]['word'], lemma_docs)\n","        for sentiment in lemma_sentiments}\n","\n","print(len(sentiment_tfidf['Negative']))\n","print(len(sentiment_tfidf['Negative'][0]))\n","\n","for sentiment in lemma_sentiments:\n","    train_df['tfidf_' + sentiment] = list(sentiment_tfidf[sentiment])\n","    \n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyYMQN6iw5q9"},"source":["Using the TFIDF values, we'll calculate the cosine similarity and plot it over time. Implement `get_cosine_similarity` to return the cosine similarities between each tick in time. Since the input, `tfidf_matrix`, is a TFIDF vector for each time period in order, you just need to computer the cosine similarities for each neighboring vector."]},{"cell_type":"code","metadata":{"id":"DTcZKmOew5q9"},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_cosine_similarity(tfidf_matrix):\n","    \"\"\"\n","    Get cosine similarities for each neighboring TFIDF vector/document\n","\n","    Parameters\n","    ----------\n","    tfidf : 2-d Numpy Ndarray of float\n","        TFIDF sentiment for each document\n","        The first dimension is the document.\n","        The second dimension is the word.\n","\n","    Returns\n","    -------\n","    cosine_similarities : list of float\n","        Cosine similarities for neighboring documents\n","    \"\"\"\n","    #print(tfidf_matrix)\n","    return [cosine_similarity(u.reshape(1,-1), v.reshape(1,-1))[0][0].tolist() for u, v in zip(tfidf_matrix, tfidf_matrix[1:])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jd0TMEiLw5q_"},"source":["cosine_similarities = {\n","    sentiment_name: get_cosine_similarity(sentiment_values) \n","    for sentiment_name, sentiment_values in sentiment_tfidf.items()}\n","\n","print(len(cosine_similarities['Negative']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGYuRUyOw5rA"},"source":["for sentiment in lemma_sentiments:\n","    # Add 0 to the first element as there is no comparison available to a previous value\n","    cosine_similarities[sentiment].insert(0, 0)\n","    train_df['cos_sim_' + sentiment] = cosine_similarities[sentiment]\n","    \n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDj4mY8Vw5rC"},"source":["# Show corelations to target\n","fig, ax = plt.subplots(figsize=(15,6))\n","corr_columns = ['target', 'tone', 'cos_sim_Negative', 'cos_sim_Positive', 'cos_sim_Uncertainty', 'cos_sim_Litigious', 'cos_sim_StrongModal', 'cos_sim_Constraining']\n","sns.heatmap(train_df[corr_columns].astype(float).corr(), cmap=\"YlGnBu\", annot=True, fmt=\".2f\", ax=ax, vmin=0, vmax=1)\n","ax.set_title(\"Correlation of train_df\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5sM4PZIOw5rD"},"source":["### Convert target class for classification"]},{"cell_type":"code","metadata":{"id":"yB0MEB0Rw5rD"},"source":["def convert_class(x):\n","    if x == 1:\n","        return 2\n","    elif x == 0:\n","        return 1\n","    elif x == -1:\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQtZhuuqw5rE"},"source":["train_df['target'] = train_df['target'].map(convert_class)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUidl33Hw5rG"},"source":["train_df['prev_decision'] = train_df['prev_decision'].map(convert_class)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RMKdbH7w5rH"},"source":["## Modeling and Training"]},{"cell_type":"code","metadata":{"id":"8JbRwEFvlwP4"},"source":["# Use GPU\n","if IN_COLAB:\n","  torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueVU_ugyMzcN"},"source":["# Use Stratified KFold Cross Validation\n","# Training data is not so many, keep n_split <= 5\n","kfold = StratifiedKFold(n_splits=3)\n","kfold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wEax7Gmow5rJ"},"source":["def metric(y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred, average='macro')\n","    return acc, f1\n","\n","scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n","refit = 'F1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcW6hliuM439"},"source":["def train_grid_search(estimator, param_grid, scoring, refit, cv=5, verbose=1, plot=True):\n","    model = GridSearchCV(estimator, param_grid=param_grid, cv=cv, scoring=scoring, verbose=verbose, \n","                         refit=refit, n_jobs=-1, return_train_score=True)\n","    model.fit(X_train, Y_train)\n","    \n","    results = model.cv_results_\n","    best_estimator = model.best_estimator_\n","    train_scores = results['mean_train_' + refit]\n","    test_scores = results['mean_test_' + refit]\n","    train_time = results['mean_fit_time']\n","    \n","    print(\"Best Score: \", model.best_score_)\n","    print(\"Best Param: \", model.best_params_)\n","    \n","    pred_train = best_estimator.predict(X_train)\n","    pred_test = best_estimator.predict(X_test)\n","\n","    acc, f1 = metric(Y_train, pred_train)\n","    logger.info('Training - acc: %.8f, f1: %.8f' % (acc, f1))\n","    acc, f1 = metric(Y_test, pred_test)\n","    logger.info('Test - acc: %.8f, f1: %.8f' % (acc, f1))\n","        \n","    if plot:\n","        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n","        fig.suptitle(\"GridSearchCV Result\", fontsize=20)\n","        \n","        ### First plot ###\n","        ax1.plot(train_scores, test_scores, 'bo')\n","        ax1.set_title(\"Train Score v.s. Test Score\", fontsize=16)\n","        ax1.set_xlabel(\"Train Score\")\n","        ax1.set_ylabel(\"Test Score\")\n","        ax1.set_xlim(0, 1)\n","        ax1.set_ylim(0, 1)\n","        ax1.grid(True)\n","        \n","        ### Second plot ###\n","        x_param = list(param_grid.keys())[0]\n","        x_param_min = np.min(list(param_grid.values())[0])\n","        x_param_max = np.max(list(param_grid.values())[0])\n","\n","        ax2.set_title(\"Score over the first param\", fontsize=16)\n","        ax2.set_xlabel(x_param)\n","        ax2.set_ylabel(\"Score\")\n","        ax2.set_xlim(x_param_min, x_param_max)\n","        ax2.set_ylim(0, 1)\n","\n","        # Get the regular numpy array from the MaskedArray\n","        X_axis = np.array(results['param_' + x_param].data, dtype=float)\n","\n","        for scorer, color in zip(sorted(scoring), ['r', 'g']):\n","            for sample, style in (('train', '--'), ('test', '-')):\n","                sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n","                sample_score_std = results['std_%s_%s' % (sample, scorer)]\n","                ax2.fill_between(X_axis, sample_score_mean - sample_score_std,\n","                                sample_score_mean + sample_score_std,\n","                                alpha=0.1 if sample == 'test' else 0, color=color)\n","                ax2.plot(X_axis, sample_score_mean, style, color=color,\n","                        alpha=1 if sample == 'test' else 0.7,\n","                        label=\"%s (%s)\" % (scorer, sample.capitalize()))\n","\n","            best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n","            best_score = results['mean_test_%s' % scorer][best_index]\n","\n","            # Plot a dotted vertical line at the best score for that scorer marked by x\n","            ax2.plot([X_axis[best_index], ] * 2, [0, best_score],\n","                    linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n","\n","            # Annotate the best score for that scorer\n","            ax2.annotate(\"%0.2f\" % best_score,\n","                        (X_axis[best_index], best_score + 0.005))\n","\n","        ax2.legend(loc=\"best\")\n","        ax2.grid(False)\n","        \n","        ### Third plot (Learning Curve) ###\n","        # Calculate learning curve (Accuracy)\n","        lc_acc_train_sizes, lc_acc_train_scores, lc_acc_test_scores = learning_curve(\n","            best_estimator, X_train, Y_train, cv=kfold, n_jobs=-1, scoring=scoring['Accuracy'], \n","            train_sizes=np.linspace(.1, 1.0, 5))\n","        lc_acc_train_mean = np.mean(lc_acc_train_scores, axis=1)\n","        lc_acc_train_std = np.std(lc_acc_train_scores, axis=1)\n","        lc_acc_test_mean = np.mean(lc_acc_test_scores, axis=1)\n","        lc_acc_test_std = np.std(lc_acc_test_scores, axis=1)\n","        \n","        # Calculate learning curve (F1 Score)\n","        lc_f1_train_sizes, lc_f1_train_scores, lc_f1_test_scores = learning_curve(\n","            best_estimator, X_train, Y_train, cv=kfold, n_jobs=-1, scoring=scoring['F1'], \n","            train_sizes=np.linspace(.1, 1.0, 5))\n","        lc_f1_train_mean = np.mean(lc_f1_train_scores, axis=1)\n","        lc_f1_train_std = np.std(lc_f1_train_scores, axis=1)\n","        lc_f1_test_mean = np.mean(lc_f1_test_scores, axis=1)\n","        lc_f1_test_std = np.std(lc_f1_test_scores, axis=1)\n","        \n","        ax3.set_title(\"Learning Curve\", fontsize=16)\n","        ax3.set_xlabel(\"Training examples\")\n","        ax3.set_ylabel(\"Score\")\n","\n","        # Plot learning curve (Accuracy)\n","        ax3.fill_between(lc_acc_train_sizes, \n","                         lc_acc_train_mean - lc_acc_train_std,\n","                         lc_acc_train_mean + lc_acc_train_std, alpha=0.1, color=\"r\")\n","        ax3.fill_between(lc_acc_train_sizes, \n","                         lc_acc_test_mean - lc_acc_test_std,\n","                         lc_acc_test_mean + lc_acc_test_std, alpha=0.1, color=\"r\")\n","        ax3.plot(lc_acc_train_sizes, lc_acc_train_mean, 'o--', color=\"r\",\n","                 label=\"Accuracy (Train)\")\n","        ax3.plot(lc_acc_train_sizes, lc_acc_test_mean, 'o-', color=\"r\",\n","                 label=\"Accuracy (Test)\")\n","        \n","        # Plot learning curve (F1 Score)\n","        ax3.fill_between(lc_f1_train_sizes, \n","                         lc_f1_train_mean - lc_f1_train_std,\n","                         lc_f1_train_mean + lc_f1_train_std, alpha=0.1, color=\"g\")\n","        ax3.fill_between(lc_f1_train_sizes, \n","                         lc_f1_test_mean - lc_f1_test_std,\n","                         lc_f1_test_mean + lc_f1_test_std, alpha=0.1, color=\"g\")\n","        ax3.plot(lc_f1_train_sizes, lc_f1_train_mean, 'o--', color=\"g\",\n","                 label=\"F1 (Train)\")\n","        ax3.plot(lc_f1_train_sizes, lc_f1_test_mean, 'o-', color=\"g\",\n","                 label=\"F1 (Test)\")\n","\n","        ax3.legend(loc=\"best\")\n","        ax3.grid(True)\n","        \n","        plt.tight_layout(pad=3.0)\n","        plt.show()\n","        \n","        ### Confusion Matrix ###\n","        class_names = ['Lower', 'Hold', 'Raise']\n","        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n","        fig.suptitle(\"Confusion Matrix\", fontsize=20)\n","        \n","        plot_confusion_matrix(best_estimator, X_train, Y_train, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize=None, ax=ax1)\n","        ax1.set_title(\"Train Data: Actual Count\")\n","        ax1.grid(False)\n","        \n","        plot_confusion_matrix(best_estimator, X_train, Y_train, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize='all', ax=ax2)\n","        ax2.set_title=(\"Train Data: Normalized\")\n","        ax2.grid(False)\n","        \n","        plot_confusion_matrix(best_estimator, X_test, Y_test, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize=None, ax=ax3)\n","        ax3.set_title=(\"Test Data: Actual Count\")\n","        ax3.grid(False)\n","        \n","        plot_confusion_matrix(best_estimator, X_test, Y_test, display_labels=class_names, \n","                              cmap=plt.cm.Blues, normalize='all', ax=ax4)\n","        ax4.set_title(\"Test Data: Normalized\")\n","        ax4.grid(False)\n","        \n","        plt.tight_layout(pad=3.0)\n","        plt.show()\n","    \n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xf8PfNh-w5rM"},"source":["## A. Cosin Similarity\n","### Train and Test Data"]},{"cell_type":"code","metadata":{"id":"28V2R9zPw5rM"},"source":["train_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvPDh80mw5rN"},"source":["# X and Y data used\n","Y_data = train_df['target']\n","X_data = train_df[nontext_columns + ['tone', 'cos_sim_Negative', 'cos_sim_Positive', 'cos_sim_Uncertainty', \n","                                     'cos_sim_Litigious', 'cos_sim_StrongModal', 'cos_sim_Constraining']]\n","\n","# Train test split (Shuffle=False will make the test data for the most recent ones)\n","X_train, X_test, Y_train, Y_test = \\\n","model_selection.train_test_split(X_data.values, Y_data.values, test_size=0.2, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6jURx7Zw5rP"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"cfnARPgUw5rP"},"source":["# Random Forest\n","rf_clf = RandomForestClassifier()\n","\n","# Perform Grid Search\n","param_grid = {'n_estimators': np.linspace(1, 60, 10, dtype=int),\n","              'min_samples_split': [3, 10],\n","              'min_samples_leaf': [3],\n","              'max_features': [7],\n","              'max_depth': [None],\n","              'criterion': ['gini'],\n","              'bootstrap': [False]}\n","\n","rf_model = train_grid_search(rf_clf, param_grid, scoring, refit, cv=kfold, verbose=1, plot=True)\n","rf_best = rf_model.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73NGZT2lw5rQ"},"source":["# Feature Importance\n","fig, ax = plt.subplots(figsize=(10,8))\n","\n","indices = np.argsort(rf_best.feature_importances_)[::-1][:40]\n","g = sns.barplot(y=X_data.columns[indices][:40], x=rf_best.feature_importances_[indices][:40] , orient='h', ax=ax)\n","g.set_xlabel(\"Relative importance\", fontsize=12)\n","g.set_ylabel(\"Features\", fontsize=12)\n","g.tick_params(labelsize=9)\n","g.set_title(\"Feature importance\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"772Gc8grw5rR"},"source":["## B. Tfidf\n","Use Tfidf instead of cosin similarity\n","### Train and Test Data"]},{"cell_type":"code","metadata":{"id":"HXfYvp2kw5rR"},"source":["train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tRlesG0Pw5rS"},"source":["vocabulary=sentiment_dict['Negative']+sentiment_dict['Positive']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ud6S-q_0w5rU"},"source":["# X and Y data used\n","Y_data = train_df['target']\n","X_data = train_df[nontext_columns + ['tone', 'tokenized_text']]\n","\n","# Train test split (Shuffle=False will make the test data for the most recent ones)\n","X_train, X_test, Y_train, Y_test = \\\n","model_selection.train_test_split(X_data.values, Y_data.values, test_size=0.2, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pENRBVkYw5rV"},"source":["import scipy\n","def get_numeric_data(x):\n","    return [record[:-2].astype(float) for record in x]\n","\n","def get_text_data(x):\n","    return [record[-1] for record in x]\n","\n","from sklearn.preprocessing import FunctionTransformer\n","transfomer_numeric = FunctionTransformer(get_numeric_data)\n","transformer_text = FunctionTransformer(get_text_data)\n","\n","clf = Pipeline([\n","    ('features', FeatureUnion([\n","            ('numeric_features', Pipeline([\n","                ('selector', transfomer_numeric)\n","            ])),\n","             ('text_features', Pipeline([\n","                ('selector', transformer_text),\n","                ('vec', TfidfVectorizer(analyzer='word', vocabulary=vocabulary))\n","            ]))\n","         ])),\n","    ('clf', RandomForestClassifier())\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G56hxBXew5rW"},"source":["pipeline = Pipeline([\n","    ('features', FeatureUnion([\n","            ('numeric_features', Pipeline([\n","                ('selector', transfomer_numeric)\n","            ])),\n","             ('text_features', Pipeline([\n","                ('selector', transformer_text),\n","                ('vec', TfidfVectorizer(analyzer='word'))\n","            ]))\n","         ])),\n","    ('clf', RandomForestClassifier())\n","])\n","\n","# Perform Grid Search\n","param_grid = {'clf__n_estimators': np.linspace(1, 60, 10, dtype=int),\n","              'clf__min_samples_split': [3, 10],\n","              'clf__min_samples_leaf': [3],\n","              'clf__max_features': [7],\n","              'clf__max_depth': [None],\n","              'clf__criterion': ['gini'],\n","              'clf__bootstrap': [False]}\n","\n","rf_model = train_grid_search(pipeline, param_grid, scoring, refit, cv=kfold, verbose=1, plot=True)\n","rf_best = rf_model.best_estimator_\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhUkUbGsw5rX"},"source":["## C. LSTM (RNN)\n","Instead of Tfidf, use LSTM. Concatinate the lstm output and the meta data at the end and dense layer to fully connect them."]},{"cell_type":"markdown","metadata":{"id":"hum1XzT0w5rX"},"source":["### Input Data"]},{"cell_type":"code","metadata":{"id":"dPi1bAgYw5rX"},"source":["# # Split data into training and validation datasets. Use an appropriate split size.\n","\n","# split_frac = 0.8\n","\n","# split_idx = int(len(token_ids)*split_frac)\n","\n","# train_features = token_ids[:split_idx]\n","# valid_features = token_ids[split_idx:]\n","# train_labels = Y_data[:split_idx]\n","# valid_labels = Y_data[split_idx:]\n","\n","# print(\"len(token_ids): \", len(token_ids))\n","# print(\"len(train_features): \", len(train_features))\n","# print(\"len(valid_features): \", len(valid_features))\n","# print(\"len(train_labels): \", len(train_labels))\n","# print(\"len(valid_labels): \", len(valid_labels))\n","\n","# X and Y data used\n","y_data = train_df['target']\n","X_data = train_df[nontext_columns + ['tone', 'token_ids']]\n","\n","# Train test split (Shuffle=False will make the test data for the most recent ones)\n","X_train, X_valid, y_train, y_valid = \\\n","model_selection.train_test_split(X_data.values, y_data.values, test_size=0.2, shuffle=True)\n","\n","X_train_meta = get_numeric_data(X_train)\n","X_train_text = get_text_data(X_train)\n","X_valid_meta = get_numeric_data(X_valid)\n","X_valid_text = get_text_data(X_valid)\n","\n","print('Shape of train meta', len(X_train_meta))\n","print('Shape of train text', len(X_train_text))\n","print(\"Shape of valid meta \", len(X_valid_meta))\n","print(\"Shape of valid text \", len(X_valid_text))\n","\n","meta_size = len(X_train_meta[0])\n","print(\"Meta data size: \", meta_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s8XdisC5w5rY"},"source":["### Model\n","#### Embed -> RNN -> Dense -> Softmax"]},{"cell_type":"code","metadata":{"id":"JKwTVfCcw5ra"},"source":["class TextClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_size, lstm_size, dense_size, meta_size, output_size, lstm_layers=1, dropout=0.1):\n","        \"\"\"\n","        Initialize the model\n","        \"\"\"\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        self.lstm_size = lstm_size\n","        self.output_size = output_size\n","        self.lstm_layers = lstm_layers\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc1 = nn.Linear(lstm_size, dense_size)\n","        self.fc2 = nn.Linear(dense_size + meta_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def init_hidden(self, batch_size):\n","        \"\"\"\n","        Initialize the hidden state\n","        \"\"\"\n","\n","        weight = next(self.parameters()).data\n","        # print('initial weight size: ', weight.shape)\n","        # print('initial weight: ', weight)\n","        # print('initial weight new: ', weight.new(self.lstm_layers, batch_size, self.lstm_size))\n","\n","        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n","                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n","        \n","        return hidden\n","\n","    def forward(self, nn_input_text, nn_input_meta, hidden_state):\n","        \"\"\"\n","        Perform a forward pass of the model on nn_input\n","        \"\"\"\n","        batch_size = nn_input_text.size(0)\n","        nn_input_text = nn_input_text.long()\n","        embeds = self.embedding(nn_input_text)\n","        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n","        # Stack up LSTM outputs, apply dropout\n","        lstm_out = lstm_out[-1,:,:]\n","        lstm_out = self.dropout(lstm_out)\n","        # Dense layer\n","        dense_out = self.fc1(lstm_out)\n","        # Concatinate the dense output and meta inputs\n","        concat_layer = torch.cat((dense_out, nn_input_meta.float()), 1)\n","        out = self.fc2(concat_layer)\n","        logps = self.softmax(out)\n","\n","        return logps, hidden_state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QPYvhIfQw5rb"},"source":["### DataLoaders and Batching\n","\n","can use keras functions but let's do it from scratch"]},{"cell_type":"code","metadata":{"id":"ouxBzJZow5rb"},"source":["# from keras.preprocessing.text import Tokenizer\n","# from keras.preprocessing.sequence import pad_sequences\n","\n","# MAX_LEN = 100\n","# tokenizer_obj = Tokenizer()\n","# tokenizer_obj.fit_on_texts(balanced['texts'])\n","# sequences = tokenizer_obj.texts_to_sequences(balanced['texts'])\n","\n","# text_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n","# text_pad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJHzJRWUw5rc"},"source":["def dataloader(messages, meta, labels, sequence_length=200, batch_size=16, shuffle=False):\n","    \"\"\" \n","    Build a dataloader.\n","    \"\"\"\n","    if shuffle:\n","        indices = list(range(len(messages)))\n","        random.shuffle(indices)\n","        messages = [messages[idx] for idx in indices]\n","        meta = [meta[idx] for idx in indices]\n","        labels = [labels[idx] for idx in indices]\n","\n","    total_sequences = len(messages)\n","\n","    for ii in range(0, total_sequences, batch_size):\n","        batch_messages = messages[ii: ii+batch_size]\n","        \n","        # First initialize a tensor of all zeros\n","        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n","        for batch_num, tokens in enumerate(batch_messages):\n","            token_tensor = torch.tensor(tokens)\n","            # print(len(tokens))\n","            # print(len(tokens[0]))\n","            # print(token_tensor.shape)\n","            # Left pad!\n","            start_idx = max(sequence_length - len(token_tensor), 0)\n","            # print(token_tensor[:sequence_length].shape)\n","            # print(start_idx, batch_num)\n","            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n","        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n","        meta_tensor = torch.tensor(meta[ii: ii+len(batch_messages)])\n","        \n","        yield batch, meta_tensor, label_tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BlqJpokyw5re"},"source":["# Test\n","text_batch, meta_batch, labels = next(iter(dataloader(X_train_text, X_train_meta, y_train)))\n","model = TextClassifier(len(vocab), 512, 128, 8, meta_size, 3)\n","hidden = model.init_hidden(16)\n","logps, hidden = model.forward(text_batch, meta_batch, hidden)\n","print(logps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adhXylejw5rf"},"source":["### Configure the model and train!"]},{"cell_type":"code","metadata":{"id":"WjRn-zjKw5rf"},"source":["# Set model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = TextClassifier(len(vocab)+1, 512, 128, 8, meta_size, 3, lstm_layers=2, dropout=0.2)\n","model.embedding.weight.data.uniform_(-1, 1)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBQuGZuKw5rg"},"source":["def train_model(model, epochs=3, batch_size=8, learning_rate=1e-4, sequence_length=200, clip=5, print_every=950):\n","  criterion = nn.NLLLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","  model.train()\n","\n","  for epoch in range(epochs):\n","    print('Starting epoch {}'.format(epoch + 1))\n","    hidden = model.init_hidden(batch_size)\n","    steps = 0\n","    y_valid_epoch = []\n","    predicted_valid_epoch = []\n","    \n","    for text_batch, meta_batch, labels in dataloader(X_train_text, X_train_meta, y_train, batch_size=batch_size, sequence_length=sequence_length, shuffle=False):\n","      steps += 1\n","      # Skip the last batch of which size is not equal to batch_size\n","      if text_batch.size(1) != batch_size:\n","        break\n","\n","      # Creating new variables for the hidden state to avoid backprop entire training history\n","      hidden = tuple([each.data for each in hidden])\n","  \n","      # Set Device\n","      text_batch, meta_batch, labels = text_batch.to(device), meta_batch.to(device), labels.to(device)\n","      for each in hidden:\n","          each.to(device)\n","  \n","      # optimizer.zero_grad()\n","      model.zero_grad()\n","  \n","      # Get output and hidden state from the model\n","      output, hidden = model(text_batch, meta_batch, hidden)\n","\n","      # Calculate the loss and perform backprop\n","      loss = criterion(output, labels)\n","      loss.backward()\n","\n","      # Clip the gradient to prevent the exploading gradient problem in RNN/LSTM\n","      nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","      # Optimize\n","      optimizer.step()\n","\n","      if steps % print_every == 0:\n","        model.eval()\n","        valid_losses = []\n","        accuracy = []\n","        predicted_valid = []\n","        y_valid_batch = []\n","        valid_hidden = model.init_hidden(batch_size)\n","        \n","        for text_batch, meta_batch, labels in dataloader(X_valid_text, X_valid_meta, y_valid, batch_size=batch_size, sequence_length=sequence_length, shuffle=False):\n","          # Skip the last batch of which size is not equal to batch_size\n","          if text_batch.size(1) != batch_size:\n","            break\n","\n","          # Initialize within the loop to use label shape because batch_size did not work\n","          # valid_hidden = model.init_hidden(labels.shape[0])\n","\n","          # Creating new variables for the hidden state\n","          valid_hidden = tuple([each.data for each in valid_hidden])\n","\n","          # Set Device\n","          text_batch, meta_batch, labels = text_batch.to(device), meta_batch.to(device), labels.to(device)\n","          for each in valid_hidden:\n","              each.to(device)\n","\n","          # Get output and hidden state from the model\n","          valid_output, valid_hidden = model(text_batch, meta_batch, valid_hidden)\n","\n","          # Calculate the loss     \n","          valid_loss = criterion(valid_output.squeeze(), labels)\n","          valid_losses.append(valid_loss.item())\n","\n","          # Accuracy\n","          ps = torch.exp(valid_output)\n","          top_p, top_class = ps.topk(1, dim=1)\n","          equals = top_class == labels.view(*top_class.shape)\n","          accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n","\n","          predicted_valid.extend(top_class.squeeze().cpu().numpy())\n","          y_valid_batch.extend(labels.view(*top_class.shape).squeeze().cpu().numpy())\n","\n","          model.train()\n","          acc, f1 = metric(y_valid_batch, predicted_valid)\n","          predicted_valid_epoch.extend(predicted_valid)\n","          y_valid_epoch.extend(y_valid_batch)\n","\n","          print(\"Epoch: {}/{}...\".format(epoch+1, epochs), \"Step: {}...\".format(steps), \"Loss: {:.6f}...\".format(loss.item()), \"Val Loss: {:.6f}\".format(np.mean(valid_losses)), \"Accuracy: {:.6f}\".format(acc), \"F1 Score: {:.6f}\".format(f1))\n","        print(\"{} steps in epoch {}\".format(steps, epoch+1))\n","        class_names = ['Lower', 'Hold', 'Raise']\n","        y_valid_class = [class_names[int(idx)] for idx in y_valid_batch]\n","        predicted_valid_class = [class_names[int(idx)] for idx in predicted_valid]\n","        titles_options = [(\"Confusion matrix, without normalization\", None), (\"Confusion matrix, with normalization\", 'true')]\n","        for title, normalize in titles_options:\n","            disp = skplt.metrics.plot_confusion_matrix(y_valid_class, predicted_valid_class, normalize=normalize, title=title)\n","        acc, f1 = metric(y_valid_class, predicted_valid_class)\n","        print(\"\\nEpoch: %d, Average Accuracy: %.8f, Average f1: %.8f\\n\" % (epoch+1, acc, f1))\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8BsRuAUNJ_z"},"source":["#train_model(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ijAMPqXcw5ri"},"source":["## D. Glove Word Embedding + LSTM\n","Use GloVe word embedding instead of Tfidf"]},{"cell_type":"code","metadata":{"id":"W0t79Uhrw5ri"},"source":["glove_file = 'glove.6B.300d.pickle'\n","glove_path = glove_dir + glove_file\n","embedding_dict = {}\n","with open(\"/content/drive/My Drive/Colab Notebooks/proj2/src/data/GloVe/glove.6B.300d.txt\", 'r') as f:\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    vectors = np.asarray(values[1:], 'float32')\n","    embedding_dict[word] = vectors\n","f.close()\n","pickle.dump(embedding_dict, open(glove_path, 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOr-yBBvw5rk","scrolled":true},"source":["glove_dict = pickle.load(open(glove_path, 'rb'))\n","print(len(glove_dict))\n","glove_dict['the']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3Xt5fehw5rl"},"source":["weight_matrix = np.zeros((len(vocab), 300))\n","words_found = 0\n","\n","for i, word in enumerate(vocab):\n","    try:\n","        weight_matrix[i] = glove_dict[word]\n","        words_found += 1\n","    except KeyError:\n","        weight_matrix[i] = np.random.normal(scale=0.6, size=(300,))\n","\n","print('{} words found out of {} words in vocab.'.format(words_found, len(vocab)))\n","print(weight_matrix.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxHMnQyAw5rm"},"source":["type(weight_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5_TAcJDw5rn"},"source":["class GloveTextClassifier(nn.Module):\n","    def __init__(self, weight_matrix, lstm_size, dense_size, meta_size, output_size, lstm_layers=1, dropout=0.1):\n","        super().__init__()\n","        vocab_size, embed_size = weight_matrix.shape\n","        self.lstm_size = lstm_size\n","        self.output_size = output_size\n","        self.lstm_layers = lstm_layers\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.embedding.load_state_dict({'weight': torch.tensor(weight_matrix)})\n","        self.embedding.weight.requires_grad = False\n","        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc1 = nn.Linear(lstm_size, dense_size)\n","        self.fc2 = nn.Linear(dense_size + meta_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n","                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n","        \n","        return hidden\n","\n","    def forward(self, nn_input_text, nn_input_meta, hidden_state):\n","        batch_size = nn_input_text.size(0)\n","        nn_input_text = nn_input_text.long()\n","        embeds = self.embedding(nn_input_text)\n","        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n","        # Stack up LSTM outputs, apply dropout\n","        lstm_out = lstm_out[-1,:,:]\n","        lstm_out = self.dropout(lstm_out)\n","        # Dense layer\n","        dense_out = self.fc1(lstm_out)\n","        # Concatinate the dense output and meta inputs\n","        concat_layer = torch.cat((dense_out, nn_input_meta.float()), 1)\n","        out = self.fc2(concat_layer)\n","        logps = self.softmax(out)\n","\n","        return logps, hidden_state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sp5s594vw5rq"},"source":["### Configure the model and train!"]},{"cell_type":"code","metadata":{"id":"0chGQdVpw5rq"},"source":["# Set model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GloveTextClassifier(weight_matrix, 128, 8, meta_size, 3, lstm_layers=2, dropout=0.2)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQ9H-nrqw5rr","scrolled":false},"source":["train_model(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GRicodvSw5ru"},"source":["The result does not look good. In fact, only the first hunderds of text can be used. Now, consider to split the text to the length of 200 with overlapping 50 words again."]},{"cell_type":"code","metadata":{"id":"Ov5FWnndw5ru"},"source":["train_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFW2T_lDw5rv"},"source":["split_train_df = train_df.drop(columns=['statement',\n","       'minutes', 'speech', 'testimony',\n","       'tokenized', 'token_ids', 'tokenized_text', 'tfidf_Negative',\n","       'tfidf_Positive', 'tfidf_Uncertainty', 'tfidf_Litigious',\n","       'tfidf_StrongModal', 'tfidf_Constraining', 'cos_sim_Negative',\n","       'cos_sim_Positive', 'cos_sim_Uncertainty', 'cos_sim_Litigious',\n","       'cos_sim_StrongModal', 'cos_sim_Constraining'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfopnhHyw5rw"},"source":["split_train_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKhCB0YLw5rx"},"source":["# Split functions to process long text in machine learning based NLP\n","\n","def get_split(text, split_len=200, overlap=50):\n","    l_total = []\n","    words = re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n","    if len(words) < split_len:\n","        n = 1\n","    else:\n","        n = (len(words) - overlap) // (split_len - overlap) + 1\n","    for i in range(n):\n","        l_parcial = words[(split_len - overlap) * i: (split_len - overlap) * i + split_len]\n","        l_total.append(\" \".join(l_parcial))\n","    return l_total\n","def get_split_df(df, split_len=200, overlap=50):\n","    split_data_list = []\n","\n","    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","        #print(\"Original Word Count: \", row['word_count'])\n","        text_list = get_split(row[\"text\"], split_len, overlap)\n","        for text in text_list:\n","            row['text'] = text\n","            #print(len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)))\n","            #row['word_count'] = len(re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text))\n","            split_data_list.append(list(row))\n","    split_df = pd.DataFrame(split_data_list, columns=df.columns)\n","    return split_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxppGF6_w5ry"},"source":["split_train_df = get_split_df(split_train_df)\n","split_train_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-KQLK4cw5rz"},"source":["tokenized = tokenize_df(split_train_df)\n","lemma_docs = [\" \".join(words) for words in tokenized]\n","all_words = [word for text in tokenized for word in text]\n","counts = Counter(all_words)\n","bow = sorted(counts, key=counts.get, reverse=True)\n","vocab = {word: ii for ii, word in enumerate(counts, 1)}\n","id2vocab = {v: k for k, v in vocab.items()}\n","token_ids = [[vocab[word] for word in text_words] for text_words in tokenized]\n","\n","# Add to the dataframe\n","split_train_df['token_ids'] = token_ids\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLhR0wdvw5r0"},"source":["weight_matrix = np.zeros((len(vocab)+1, 300))\n","words_found = 0\n","\n","for i, word in enumerate(vocab):\n","    try:\n","        weight_matrix[i] = glove_dict[word]\n","        words_found += 1\n","    except KeyError:\n","        weight_matrix[i] = np.random.normal(scale=0.6, size=(300,))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xHTHM3zw5r1"},"source":["split_train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IzDilivbw5r2"},"source":["sns.distplot(split_train_df['text'].apply(lambda x: len(x.split())))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hkZUuwVrw5r3"},"source":["# X and Y data used\n","y_data = split_train_df['target']\n","X_data = split_train_df[nontext_columns + ['tone', 'token_ids']]\n","\n","# Train test split (Shuffle=False will make the test data for the most recent ones)\n","X_train, X_valid, y_train, y_valid = \\\n","model_selection.train_test_split(X_data.values, y_data.values, test_size=0.2, shuffle=True)\n","\n","X_train_meta = get_numeric_data(X_train)\n","X_train_text = get_text_data(X_train)\n","X_valid_meta = get_numeric_data(X_valid)\n","X_valid_text = get_text_data(X_valid)\n","\n","print('Shape of train meta', len(X_train_meta))\n","print('Shape of train text', len(X_train_text))\n","print(\"Shape of valid meta \", len(X_valid_meta))\n","print(\"Shape of valid text \", len(X_valid_text))\n","\n","meta_size = len(X_train_meta[0])\n","print(\"Meta data size: \", meta_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUQ5hvIYw5r4"},"source":["len(weight_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ulmapnRw5r5"},"source":["# Set model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GloveTextClassifier(weight_matrix, 128, 8, meta_size, 3, lstm_layers=2, dropout=0.2)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJPVLhVjNRrc"},"source":["train_model(model, epochs=3, batch_size=16, learning_rate=1e-4, sequence_length=200, clip=5, print_every=950) #TODO: change print_every to get graphs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Fd44Xjhw5r9"},"source":["## E. BERT Model"]},{"cell_type":"code","metadata":{"id":"Q7VHK6bSNTzN"},"source":["from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","#from transformers import *\n","from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n","#import torch\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNewaN-xw5r-"},"source":["class InputFeature(object):\n","    def __init__(self, id, input_ids, masks, segments, meta, label=None):\n","        self.id = id\n","        self.features = {\n","            'input_ids': input_ids,\n","            'input_mask': masks,\n","            'segment_ids': segments,\n","            'meta': meta\n","        }\n","        self.label = label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAz_0B0Bw5r_"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def bert_encoder(text, max_len=200):\n","    text_token = tokenizer.tokenize(text)\n","    text_token = text_token[:max_len-2]\n","    text_token = [\"[CLS]\"] + text_token + [\"[SEP]\"]\n","    text_ids = tokenizer.convert_tokens_to_ids(text_token)\n","    text_ids += [0] * (max_len - len(text_token))\n","    pad_masks = [1] * len(text_token) + [0] * (max_len - len(text_token))\n","    segment_ids = [0] * len(text_token) + [0] * (max_len - len(text_token))\n","    \n","    return text_ids, pad_masks, segment_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSwZ4QMrw5sB"},"source":["train_set = []\n","max_seq_length = 200\n","meta_size = 10\n","\n","for index, row in tqdm(split_train_df.iterrows(), total=split_train_df.shape[0]):\n","    input_ids, masks, segments = bert_encoder(row['text'], max_seq_length)\n","    train_set.append(InputFeature(row.index, input_ids, masks, segments, row[nontext_columns + ['tone']], int(row['target'])))    \n","\n","train_labels = split_train_df['target'].astype(int).values\n","train_valid_input_ids = np.array([data.features['input_ids'] for data in train_set])\n","train_valid_input_masks = np.array([data.features['input_mask'] for data in train_set])\n","train_valid_segment_ids =np.array([data.features['segment_ids'] for data in train_set])\n","train_valid_meta =np.array([data.features['meta'] for data in train_set], dtype=np.float64)\n","train_valid_labels = np.array([data.label for data in train_set])\n","\n","oof_train = np.zeros((len(split_train_df), 3), dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rPQQBcmwihw"},"source":["print(train_valid_meta[0])\n","print(train_valid_meta[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"diYHe05tw5sC"},"source":["class BertTextClassifier(nn.Module):\n","    def __init__(self, hidden_size, dense_size, meta_size, output_size, dropout=0.1):\n","        super().__init__()\n","        self.output_size = output_size\n","        self.dropout = dropout\n","        \n","        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n","                                        output_hidden_states=True,\n","                                        output_attentions=True)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.weights = nn.Parameter(torch.rand(13, 1))\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc1 = nn.Linear(hidden_size, dense_size)\n","        self.fc2 = nn.Linear(dense_size + meta_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input_ids, nn_input_meta):\n","        all_hidden_states, all_attentions = self.bert(input_ids)[-2:]\n","        batch_size = input_ids.shape[0]\n","        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n","        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n","        atten = F.softmax(atten.view(-1), dim=0)\n","        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])        \n","        # Dense layer\n","        dense_out = self.fc1(self.dropout(feature))\n","        # Concatinate the dense output and meta inputs\n","        concat_layer = torch.cat((dense_out, nn_input_meta.float()), 1)\n","        # print(len(dense_out[0]))\n","        # print(len(nn_input_meta[0]))\n","        # print(len(concat_layer[0]))\n","        # print(\"dense_out: \\n\", dense_out)\n","        # print(\"nn_input_meta: \\n\", nn_input_meta)\n","        # print(\"concat_layer: \\n\", concat_layer)\n","        out = self.fc2(concat_layer)\n","        #logps = self.softmax(out)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5mv5KIZw5sD"},"source":["# Check how BertTokenizer works\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n","outputs = model(input_ids)\n","\n","print(input_ids)\n","print(outputs) # The last hidden-state is the first element of the output tuple\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwdZf51Gw5sE","scrolled":false},"source":["# Test Tokenizer - Own Implementation\n","bert_model = BertTextClassifier(768, 128, meta_size, 3, dropout=0.1)\n","\n","text_ids, pad_masks, segment_ids = bert_encoder(\"Hello, my dog is cute\")\n","print('text_ids: \\n', text_ids)\n","print('text_ids (torch.tensor): \\n', torch.tensor(text_ids))\n","text_ids = torch.tensor(text_ids).unsqueeze(0)\n","print('text_ids (unsqueezed): \\n', text_ids)\n","#print('pad_masks: ',pad_masks)\n","#print('segment_ids: ',segment_ids)\n","x_meta = torch.tensor([1,2,3,4,5,6,7,8,9,10]).unsqueeze(0)\n","outputs = bert_model(text_ids, x_meta)\n","print(len(outputs))\n","print('outputs: \\n',outputs)\n","print('outputs(detached): \\n', outputs.detach())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"peYs1C_7w5sF"},"source":["# Hyperparameters\n","learning_rate = 1e-5\n","num_epochs = 3\n","batch_size = 32\n","patience =2\n","file_name = 'model'\n","use_skf = True\n","bert_hidden_size = 768\n","bert_dense_size =128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1gi8IEAw5sF"},"source":["def train_bert(fold, train_indices, valid_indices):\n","    \n","    # Number of folds to iterrate\n","    # if fold == 3:\n","    #     break\n","\n","    logger.info('================     fold {}        ==============='.format(fold))\n","    \n","    # Train Data in Tensor\n","    train_input_ids = torch.tensor(train_valid_input_ids[train_indices], dtype=torch.long)\n","    train_input_mask = torch.tensor(train_valid_input_masks[train_indices], dtype=torch.long)\n","    train_segment_ids = torch.tensor(train_valid_segment_ids[train_indices], dtype=torch.long)\n","    train_label = torch.tensor(train_valid_labels[train_indices], dtype=torch.long)\n","    train_meta = torch.tensor(train_valid_meta[train_indices], dtype=torch.long)\n","    \n","    # Validation Data in Tensor\n","    valid_input_ids = torch.tensor(train_valid_input_ids[valid_indices], dtype=torch.long)\n","    valid_input_mask = torch.tensor(train_valid_input_masks[valid_indices], dtype=torch.long)\n","    valid_segment_ids = torch.tensor(train_valid_segment_ids[valid_indices], dtype=torch.long)\n","    valid_label = torch.tensor(train_valid_labels[valid_indices], dtype=torch.long)\n","    valid_meta = torch.tensor(train_valid_meta[valid_indices], dtype=torch.long)\n","\n","    # Load data into TensorDataset\n","    train = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_meta, train_label)\n","    valid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_meta, valid_label)\n","    \n","    # Use DataLoader to load data from Dataset in batches\n","    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n","    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n","\n","    # Set Model\n","#     bert_model = BertForSequenceClassification.from_pretrained(\n","#         \"bert-base-uncased\",\n","#         num_labels = 3,\n","#         output_attentions = False,\n","#         output_hidden_states = False\n","#     )\n","\n","    bert_model = BertTextClassifier(bert_hidden_size, bert_dense_size, meta_size, 3, dropout=0.1)\n","    \n","    # Move model to GUP/CPU device\n","    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","    bert_model = bert_model.to(device)\n","    \n","    # Loss Function - use Cross Entropy as binary classification\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    # Optimizer - Adam with parameter groups\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-6)\n","    \n","    # Set Train Mode\n","    bert_model.train()\n","\n","    # Initialize\n","    best_f1 = 0.\n","    valid_best = np.zeros((valid_label.size(0), 2))\n","    early_stop = 0\n","    train_losses = []\n","    valid_losses = []\n","    \n","    for epoch in range(num_epochs):\n","        logger.info('================     epoch {}        ==============='.format(epoch+1))\n","        train_loss = 0.\n","        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc='Training'):\n","            # Move batch data to device\n","            batch = tuple(t.to(device) for t in batch)\n","            # Bert input features and labels from batch\n","            x_ids, x_mask, x_sids, x_meta, y_truth = batch\n","            \n","            # Feedforward prediction\n","            y_pred = bert_model(x_ids, x_meta)\n","\n","            # Calculate Loss\n","            loss = loss_fn(y_pred, y_truth)\n","                        \n","            # Reset gradient\n","            optimizer.zero_grad()\n","            # Backward Propagation\n","            loss.backward()\n","            # Update Weights\n","            optimizer.step()\n","            # Training Loss\n","            train_loss += loss.item() / len(train_loader)\n","            \n","            logger.debug('train batch: %d, train_loss: %8f\\n' % (i, train_loss))\n","\n","        train_losses.append(train_loss)\n","        # Move to Evaluation Mode\n","        model.eval()\n","        \n","        # Initialize\n","        val_loss = 0.\n","        valid_preds_fold = np.zeros((valid_label.size(0), 3))\n","        \n","        with torch.no_grad():\n","            for i, batch in tqdm(enumerate(valid_loader), total=len(valid_loader), desc='Validation'):\n","                batch = tuple(t.to(device) for t in batch)\n","                x_ids, x_mask, x_sids, x_meta, y_truth = batch\n","                y_pred = bert_model(x_ids, x_meta).detach()\n","                loss = loss_fn(y_pred, y_truth)\n","                val_loss += loss.item() / len(valid_loader)\n","                valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n","                \n","                logger.debug('validation batch: {}, val_loss: {}, valid_preds_fold: {}'.format(i, val_loss, valid_preds_fold[i * batch_size:(i + 1) * batch_size]))\n","            valid_losses.append(val_loss)\n","\n","        # Calculate metrics\n","        acc, f1 = metric(train_valid_labels[valid_indices], np.argmax(valid_preds_fold, axis=1))\n","        \n","        # If improving, save the model. If not, count up for early stopping\n","        if best_f1 < f1:\n","            early_stop = 0\n","            best_f1 = f1\n","            valid_best = valid_preds_fold\n","            torch.save(bert_model.state_dict(), output_dir + 'model_fold_{}.dict'.format(fold))\n","        else:\n","            early_stop += 1\n","            \n","        logger.info(\n","            'epoch: %d, train loss: %.8f, valid loss: %.8f, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n","            (epoch, train_loss, val_loss, acc, f1, best_f1))\n","        \n","        if device == 'cuda:0':\n","            torch.cuda.empty_cache()  \n","\n","        # Early stop if it reaches patience number\n","        if early_stop >= patience:\n","            break\n","            \n","        model.train()\n","\n","    # Once all epochs are done, take the best model of the fold\n","    valid_preds_fold = np.zeros((valid_label.size(0), 3))\n","    \n","    # Draw training/validation losses\n","    sns.set(font_scale=1.5)\n","    plt.rcParams[\"figure.figsize\"] = (15,6)\n","    plt.plot(train_losses, 'b-o')\n","    plt.plot(valid_losses, 'b-o')\n","\n","    plt.title(\"Training/Validation Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","\n","    plt.show()\n","\n","    # Load the best model\n","    bert_model.load_state_dict(torch.load(output_dir + 'model_fold_{}.dict'.format(fold)))\n","    # Set Evaluation Mode\n","    bert_model.eval()\n","    \n","    # Prediction on the validation set\n","    with torch.no_grad():\n","        for i, batch in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n","            batch = tuple(t.to(device) for t in batch)\n","            x_ids, x_mask, x_sids, x_meta, y_truth = batch\n","            y_pred = bert_model(x_ids, x_meta).detach()\n","            valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n","\n","    # Check the metrics for the validation set\n","    valid_best = valid_preds_fold\n","    oof_train[valid_indices] = valid_best\n","    acc, f1 = metric(train_valid_labels[valid_indices], np.argmax(valid_best, axis=1))\n","    logger.info('epoch: best, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' % (acc, f1, best_f1))\n","\n","    class_names = ['Lower', 'Hold', 'Raise']\n","    titles_options = [(\"Confusion matrix, without normalization\", None), (\"Normalized confusion matrix\", 'true')]\n","    for title, normalize in titles_options:\n","        disp = skplt.metrics.plot_confusion_matrix(train_valid_labels[valid_indices], np.argmax(valid_best, axis=1), normalize=normalize, title=title)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcSDa10Gw5sG","scrolled":false},"source":["if use_skf:\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","  \n","    for fold, (train_indices, valid_indices) in enumerate(skf.split(train_valid_labels, train_valid_labels)):\n","        train_bert(fold, train_indices, valid_indices)\n","\n","else:\n","    train_ratio = 0.7\n","    train_indices = np.arange(0, int(len(train_valid_labels)*train_ratio))\n","    valid_indices = np.arange(int(len(train_valid_labels)*train_ratio), len(train_valid_labels))\n","\n","    train_bert(0, train_indices, valid_indices)\n","    # print('train_indices', train_indices)\n","    # print('valid_indices', valid_indices)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXMbcpK_w5sH"},"source":["# This is only when all folds have been performed\n","logger.info(f1_score(train_labels, np.argmax(oof_train, axis=1), average='macro'))\n","split_train_df['pred_target'] = np.argmax(oof_train, axis=1)\n","split_train_df['pred_target_lower'] = oof_train[:,0]\n","split_train_df['pred_target_hold'] = oof_train[:,1]\n","split_train_df['pred_target_raise'] = oof_train[:,2]\n","split_train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EdEn3xMNw5sI"},"source":["# Save Data"]},{"cell_type":"code","metadata":{"id":"mqwUOnPtw5sJ"},"source":["def save_data(df, file_name, dir_name=train_dir):\n","    if not os.path.exists(dir_name):\n","        os.mkdir(dir_name)\n","        \n","    # Save results to a picke file\n","    file = open(dir_name + file_name + '.pickle', 'wb')\n","    pickle.dump(df, file)\n","    file.close()\n","\n","    # Save results to a csv file\n","    df.to_csv(dir_name + file_name + '.csv', index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lynjDKNCw5sJ"},"source":["# Save text data\n","save_data(train_df, 'train_df')\n","save_data(text_df, 'text_df')\n","save_data(train_df, 'split_train_df')"],"execution_count":null,"outputs":[]}]}