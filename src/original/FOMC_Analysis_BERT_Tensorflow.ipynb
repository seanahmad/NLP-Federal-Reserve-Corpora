{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"FOMC_Analysis_BERT_Tensorflow.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"xQwhoyOWsm_j"},"source":["# Predict Hike/Keep/Lower from FOMC documents\n","## Prep work\n","### Install/Import necessary libraries"]},{"cell_type":"code","metadata":{"id":"-z1LUHSetAze","executionInfo":{"status":"ok","timestamp":1605147263991,"user_tz":300,"elapsed":18629,"user":{"displayName":"Theo Dimitrasopoulos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuB53M883a3xrW9cW6pSshf74SDvre3MBUkMijuw=s64","userId":"09079511126649814689"}},"outputId":"156c41bf-2335-4e34-847a-eb77758eb42f","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7EaNQqQ4sm_l","executionInfo":{"status":"ok","timestamp":1605147275898,"user_tz":300,"elapsed":30493,"user":{"displayName":"Theo Dimitrasopoulos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuB53M883a3xrW9cW6pSshf74SDvre3MBUkMijuw=s64","userId":"09079511126649814689"}},"outputId":"071a1bd7-d69a-491f-b781-5450e100551a","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install tensorflow\n","!pip install tensorflow_hub\n","!pip install sentencepiece\n","!pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n","Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n","Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.6/dist-packages (0.10.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (3.12.4)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.18.5)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (50.3.2)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 4.5MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.94\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hmK0vTursm_n","executionInfo":{"status":"ok","timestamp":1605147282840,"user_tz":300,"elapsed":37427,"user":{"displayName":"Theo Dimitrasopoulos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuB53M883a3xrW9cW6pSshf74SDvre3MBUkMijuw=s64","userId":"09079511126649814689"}},"outputId":"e51b99c0-6f14-4368-aa7f-c059d1b4a55d","colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","import random\n","\n","from collections import defaultdict\n","from collections import Counter\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","plt.style.use('ggplot')\n","stop = set(stopwords.words('english'))\n","\n","import datetime as dt\n","import time\n","import re\n","import pickle\n","import logging\n","from tqdm import tqdm_notebook\n","import torch\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import tensorflow_hub as hub\n","!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n","import tokenization"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CG-07Fm0sm_q"},"source":["logger = logging.getLogger('mylogger')\n","logger.setLevel(logging.DEBUG)\n","timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n","fh = logging.FileHandler('log_model.txt')\n","fh.setLevel(logging.DEBUG)\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.DEBUG)\n","formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n","fh.setFormatter(formatter)\n","ch.setFormatter(formatter)\n","logger.addHandler(fh)\n","logger.addHandler(ch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OGSOqKVsm_s"},"source":["# Set Random Seed\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOYfY193sm_u"},"source":["### Load Train Set"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VQ101Npxsm_u","executionInfo":{"status":"error","timestamp":1605147283334,"user_tz":300,"elapsed":37911,"user":{"displayName":"Theo Dimitrasopoulos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuB53M883a3xrW9cW6pSshf74SDvre3MBUkMijuw=s64","userId":"09079511126649814689"}},"outputId":"e69f0486-cecb-407a-d945-3ef40aa786b6","colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["# Load data\n","file = open('C:/Users/theon/Desktop/proj2/data/FOMC/train_split_df.pickle', 'rb')\n","\n","df = pickle.load(file)\n","file.close()\n","\n","df"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c0c0d866ef20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/theon/Desktop/proj2/data/FOMC/train_split_df.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/theon/Desktop/proj2/data/FOMC/train_split_df.pickle'"]}]},{"cell_type":"markdown","metadata":{"id":"LClnSjDhsm_w"},"source":["## EDA"]},{"cell_type":"code","metadata":{"id":"76pcwRwzsm_x"},"source":["x = df.target.value_counts()\n","sns.barplot(x.index, x)\n","plt.gca().set_ylabel('number of samples')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl8br_3Rsm_z"},"source":["sns.countplot(x='target', data=df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6lm7X2ssm_1"},"source":["def create_corpus(target):\n","    corpus = []\n","    \n","    for x in df[df['target']==target]['contents'].str.split():\n","        for i in x:\n","            corpus.append(i)\n","    return corpus\n","\n","def get_frequent_words(corpus, top_n=10):\n","    dic = defaultdict(int)\n","    for word in corpus:\n","        if word in stop:\n","            dic[word] += 1\n","\n","    top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:top_n]\n","\n","    return zip(*top)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRulwELdsm_3"},"source":["# Most frequent words\n","corpus_lower = create_corpus(-1)\n","corpus_hold = create_corpus(0)\n","corpus_raise = create_corpus(1)\n","\n","fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,7))\n","x, y = get_frequent_words(corpus_lower)\n","ax1.bar(x, y)\n","ax1.set_title('Lower')\n","x, y = get_frequent_words(corpus_hold)\n","ax2.bar(x, y)\n","ax2.set_title('Hold')\n","x, y = get_frequent_words(corpus_raise)\n","ax3.bar(x, y)\n","ax3.set_title('Raise')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjeGW3IHsm_5"},"source":["# Check most frequent words which are not in stopwords\n","counter = Counter(corpus_lower)\n","most = counter.most_common()\n","x, y = [], []\n","for word, count in most[:40]:\n","    if word not in stop:\n","        x.append(word)\n","        y.append(count)\n","\n","plt.figure(figsize=(10,5))\n","sns.barplot(x=y, y=x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHpJB-f-sm_7"},"source":["fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15,15))\n","contents_len = df[df['type'] == 'statement']['word_count']\n","ax1.hist(contents_len, color='red', bins=100, range=(0,20000))\n","ax1.set_title('Statement')\n","contents_len = df[df['type'] == 'minutes']['word_count']\n","ax2.hist(contents_len, color='green', bins=100, range=(0,20000))\n","ax2.set_title('Minutes')\n","contents_len = df[df['type'] =='script']['word_count']\n","ax3.hist(contents_len, color='blue', bins=100, range=(0,60000))\n","ax3.set_title('Script')\n","contents_len = df[df['type'] =='speech']['word_count']\n","ax4.hist(contents_len, color='brown', bins=100, range=(0,20000))\n","ax4.set_title('Speech')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6cb8Ngjsm_9"},"source":["## Tokenize"]},{"cell_type":"code","metadata":{"id":"siYk2a8zsm_9"},"source":["def bert_encode(texts, tokenizer, max_len=512):\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        text = tokenizer.tokenize(text)\n","            \n","        text = text[:max_len-2]\n","        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n","        pad_len = max_len - len(input_sequence)\n","        \n","        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n","        tokens += [0] * pad_len\n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","        segment_ids = [0] * max_len\n","        \n","        all_tokens.append(tokens)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","    \n","    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTEXr3Oqsm__"},"source":["def build_model(bert_layer, max_len=512):\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","\n","    #_, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    #clf_output = sequence_output[:, 0, :]\n","    #out = Dense(3, activation='softmax')(clf_output)\n","    \n","    _, bert_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    clf_output = bert_output[:, 0, :]\n","    out = Dense(3, activation='softmax')(clf_output) \n","    \n","    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n","    model.compile(Adam(lr=2e-6), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUNnIjqvsnAB"},"source":["%%time\n","module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n","bert_layer = hub.KerasLayer(module_url, trainable=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_n5PvITsnAD"},"source":["vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aRd55usksnAF"},"source":["# Check how BERT tokenizer tokenizes a text\n","sample = df.contents.values[0]\n","print(sample)\n","sample_tokenize = tokenizer.tokenize(sample)\n","sample_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"os6J1SI5snAH"},"source":["#train_df = df.loc[df['type'].isin(['statement', 'minutes', 'script'])]\n","train_df = df.loc[df['type'].isin(['statement'])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"btx2pU56snAJ"},"source":["train_input = bert_encode(train_df.contents.values, tokenizer, max_len=512)\n","train_labels = train_df[['target_lower', 'target_hold', 'target_raise']].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVgvY8visnAL"},"source":["list(tokenizer.vocab.keys())[5000:5020]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ReUgVrNGsnAM"},"source":["model = build_model(bert_layer, max_len=512)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7j2Qn3fsnAO"},"source":["train_history = model.fit(\n","    train_input, train_labels,\n","    validation_split=0.2,\n","    epochs=3,\n","    batch_size=16\n",")\n","\n","model.save('model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvbtnXMwsnAQ"},"source":["test_df = train_df[train_df.index > '2019-01-01']\n","test_input = bert_encode(test_df.contents.values, tokenizer, max_len=512)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnjkEawBsnAS"},"source":["train_input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NA7z7omSsnAU"},"source":["test_input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUsBGAJ7snAW"},"source":["pred = model.predict(test_input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgLFTjPqsnAY"},"source":["!pip list | grep tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jA334XAAsnAa"},"source":[""],"execution_count":null,"outputs":[]}]}